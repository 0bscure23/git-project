{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c338462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (0.30.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: trl in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (0.17.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from trl) (3.5.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from trl) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trl) (4.47.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.6.0+cu126)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from accelerate>=0.34.0->trl) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.10.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->trl) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->trl) (2.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\asus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'C:\\Users\\asus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\asus\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.6.0+cu126 requires torch==2.6.0+cu126, but you have torch 2.7.0 which is incompatible.\n",
      "torchvision 0.21.0+cu126 requires torch==2.6.0+cu126, but you have torch 2.7.0 which is incompatible.\n",
      "nanogcg 0.3.0 requires transformers<=4.47.1,>=4.4, but you have transformers 4.51.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (2.6.0+cu126)\n",
      "Collecting torch\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/44/80/b353c024e6b624cd9ce1d66dcb9d24e0294680f95b369f19280e241a0159/torch-2.7.0-cp312-cp312-win_amd64.whl (212.5 MB)\n",
      "     ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.5/212.5 MB 1.7 MB/s eta 0:02:07\n",
      "     ---------------------------------------- 0.8/212.5 MB 1.8 MB/s eta 0:02:00\n",
      "     ---------------------------------------- 1.6/212.5 MB 2.2 MB/s eta 0:01:36\n",
      "     ---------------------------------------- 2.1/212.5 MB 2.3 MB/s eta 0:01:30\n",
      "      --------------------------------------- 2.9/212.5 MB 2.6 MB/s eta 0:01:20\n",
      "      --------------------------------------- 3.7/212.5 MB 2.9 MB/s eta 0:01:13\n",
      "      --------------------------------------- 4.2/212.5 MB 3.0 MB/s eta 0:01:10\n",
      "      --------------------------------------- 4.2/212.5 MB 3.0 MB/s eta 0:01:10\n",
      "      --------------------------------------- 4.2/212.5 MB 3.0 MB/s eta 0:01:10\n",
      "      --------------------------------------- 5.0/212.5 MB 2.3 MB/s eta 0:01:31\n",
      "     - -------------------------------------- 5.8/212.5 MB 2.4 MB/s eta 0:01:26\n",
      "     - -------------------------------------- 5.8/212.5 MB 2.4 MB/s eta 0:01:26\n",
      "     - -------------------------------------- 5.8/212.5 MB 2.4 MB/s eta 0:01:26\n",
      "     - -------------------------------------- 5.8/212.5 MB 2.4 MB/s eta 0:01:26\n",
      "     - -------------------------------------- 6.3/212.5 MB 2.0 MB/s eta 0:01:45\n",
      "     - -------------------------------------- 6.3/212.5 MB 2.0 MB/s eta 0:01:45\n",
      "     - -------------------------------------- 8.1/212.5 MB 2.2 MB/s eta 0:01:32\n",
      "     - -------------------------------------- 8.1/212.5 MB 2.2 MB/s eta 0:01:32\n",
      "     - -------------------------------------- 8.9/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     - -------------------------------------- 9.4/212.5 MB 2.2 MB/s eta 0:01:32\n",
      "     - ------------------------------------- 10.0/212.5 MB 2.2 MB/s eta 0:01:32\n",
      "     - ------------------------------------- 10.2/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     - ------------------------------------- 10.2/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     - ------------------------------------- 10.2/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     -- ------------------------------------ 11.5/212.5 MB 2.1 MB/s eta 0:01:34\n",
      "     -- ------------------------------------ 11.8/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     -- ------------------------------------ 11.8/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     -- ------------------------------------ 12.1/212.5 MB 2.0 MB/s eta 0:01:40\n",
      "     -- ------------------------------------ 13.4/212.5 MB 2.2 MB/s eta 0:01:33\n",
      "     -- ------------------------------------ 14.2/212.5 MB 2.2 MB/s eta 0:01:30\n",
      "     -- ------------------------------------ 14.2/212.5 MB 2.2 MB/s eta 0:01:30\n",
      "     -- ------------------------------------ 14.2/212.5 MB 2.2 MB/s eta 0:01:30\n",
      "     -- ------------------------------------ 14.7/212.5 MB 2.1 MB/s eta 0:01:36\n",
      "     -- ------------------------------------ 15.2/212.5 MB 2.1 MB/s eta 0:01:33\n",
      "     -- ------------------------------------ 15.7/212.5 MB 2.1 MB/s eta 0:01:34\n",
      "     -- ------------------------------------ 16.0/212.5 MB 2.1 MB/s eta 0:01:35\n",
      "     -- ------------------------------------ 16.0/212.5 MB 2.1 MB/s eta 0:01:35\n",
      "     --- ----------------------------------- 16.5/212.5 MB 2.0 MB/s eta 0:01:37\n",
      "     --- ----------------------------------- 17.3/212.5 MB 2.1 MB/s eta 0:01:33\n",
      "     --- ----------------------------------- 17.6/212.5 MB 2.1 MB/s eta 0:01:35\n",
      "     --- ----------------------------------- 17.8/212.5 MB 2.0 MB/s eta 0:01:36\n",
      "     --- ----------------------------------- 18.4/212.5 MB 2.0 MB/s eta 0:01:36\n",
      "     --- ----------------------------------- 19.4/212.5 MB 2.1 MB/s eta 0:01:32\n",
      "     --- ----------------------------------- 19.7/212.5 MB 2.1 MB/s eta 0:01:32\n",
      "     --- ----------------------------------- 19.9/212.5 MB 2.1 MB/s eta 0:01:32\n",
      "     --- ----------------------------------- 19.9/212.5 MB 2.1 MB/s eta 0:01:32\n",
      "     --- ----------------------------------- 20.2/212.5 MB 2.0 MB/s eta 0:01:35\n",
      "     --- ----------------------------------- 20.4/212.5 MB 2.0 MB/s eta 0:01:37\n",
      "     --- ----------------------------------- 21.0/212.5 MB 2.0 MB/s eta 0:01:36\n",
      "     --- ----------------------------------- 21.2/212.5 MB 2.0 MB/s eta 0:01:36\n",
      "     --- ----------------------------------- 21.5/212.5 MB 2.0 MB/s eta 0:01:36\n",
      "     ---- ---------------------------------- 22.0/212.5 MB 2.0 MB/s eta 0:01:37\n",
      "     ---- ---------------------------------- 22.3/212.5 MB 2.0 MB/s eta 0:01:37\n",
      "     ---- ---------------------------------- 22.5/212.5 MB 2.0 MB/s eta 0:01:38\n",
      "     ---- ---------------------------------- 22.5/212.5 MB 2.0 MB/s eta 0:01:38\n",
      "     ---- ---------------------------------- 22.5/212.5 MB 2.0 MB/s eta 0:01:38\n",
      "     ---- ---------------------------------- 23.1/212.5 MB 1.9 MB/s eta 0:01:40\n",
      "     ---- ---------------------------------- 23.6/212.5 MB 1.9 MB/s eta 0:01:40\n",
      "     ---- ---------------------------------- 23.6/212.5 MB 1.9 MB/s eta 0:01:40\n",
      "     ---- ---------------------------------- 23.6/212.5 MB 1.9 MB/s eta 0:01:40\n",
      "     ---- ---------------------------------- 23.9/212.5 MB 1.8 MB/s eta 0:01:43\n",
      "     ---- ---------------------------------- 24.6/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 24.9/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 24.9/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 24.9/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 24.9/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 26.0/212.5 MB 1.8 MB/s eta 0:01:43\n",
      "     ---- ---------------------------------- 26.7/212.5 MB 1.9 MB/s eta 0:01:41\n",
      "     ---- ---------------------------------- 27.0/212.5 MB 1.8 MB/s eta 0:01:42\n",
      "     ----- --------------------------------- 27.3/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 27.8/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 28.0/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 28.3/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 28.3/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 28.6/212.5 MB 1.8 MB/s eta 0:01:43\n",
      "     ----- --------------------------------- 29.6/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 29.9/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 30.1/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ----- --------------------------------- 30.4/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 30.9/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 31.2/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 31.5/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 31.5/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ----- --------------------------------- 31.7/212.5 MB 1.8 MB/s eta 0:01:42\n",
      "     ----- --------------------------------- 32.0/212.5 MB 1.8 MB/s eta 0:01:42\n",
      "     ------ -------------------------------- 33.0/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 33.3/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 33.3/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 33.6/212.5 MB 1.8 MB/s eta 0:01:41\n",
      "     ------ -------------------------------- 34.3/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 34.6/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 34.9/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 35.4/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 35.7/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 36.2/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 36.4/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 36.7/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 36.7/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 37.2/212.5 MB 1.8 MB/s eta 0:01:40\n",
      "     ------ -------------------------------- 37.5/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------ -------------------------------- 38.0/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 38.3/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 38.5/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 38.8/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 39.1/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 39.6/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 39.8/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 40.1/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 40.4/212.5 MB 1.8 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 40.9/212.5 MB 1.7 MB/s eta 0:01:39\n",
      "     ------- ------------------------------- 41.2/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 41.4/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 41.9/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 42.2/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 42.5/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 42.7/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 43.3/212.5 MB 1.7 MB/s eta 0:01:38\n",
      "     ------- ------------------------------- 43.5/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 43.8/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 44.3/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 44.6/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 44.8/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 45.4/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 45.6/212.5 MB 1.7 MB/s eta 0:01:37\n",
      "     -------- ------------------------------ 45.9/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 46.4/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 46.7/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 46.9/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 47.4/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 47.7/212.5 MB 1.7 MB/s eta 0:01:36\n",
      "     -------- ------------------------------ 48.2/212.5 MB 1.7 MB/s eta 0:01:35\n",
      "     -------- ------------------------------ 48.5/212.5 MB 1.7 MB/s eta 0:01:35\n",
      "     -------- ------------------------------ 48.8/212.5 MB 1.7 MB/s eta 0:01:35\n",
      "     --------- ----------------------------- 49.3/212.5 MB 1.7 MB/s eta 0:01:35\n",
      "     --------- ----------------------------- 49.5/212.5 MB 1.7 MB/s eta 0:01:34\n",
      "     --------- ----------------------------- 50.1/212.5 MB 1.7 MB/s eta 0:01:34\n",
      "     --------- ----------------------------- 50.3/212.5 MB 1.7 MB/s eta 0:01:34\n",
      "     --------- ----------------------------- 50.9/212.5 MB 1.7 MB/s eta 0:01:34\n",
      "     --------- ----------------------------- 51.1/212.5 MB 1.7 MB/s eta 0:01:33\n",
      "     --------- ----------------------------- 51.6/212.5 MB 1.7 MB/s eta 0:01:33\n",
      "     --------- ----------------------------- 51.9/212.5 MB 1.7 MB/s eta 0:01:33\n",
      "     --------- ----------------------------- 52.4/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 52.7/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 53.2/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 53.5/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 54.0/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 54.0/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     --------- ----------------------------- 54.3/212.5 MB 1.7 MB/s eta 0:01:34\n",
      "     ---------- ---------------------------- 54.8/212.5 MB 1.7 MB/s eta 0:01:33\n",
      "     ---------- ---------------------------- 55.6/212.5 MB 1.7 MB/s eta 0:01:32\n",
      "     ---------- ---------------------------- 56.1/212.5 MB 1.7 MB/s eta 0:01:31\n",
      "     ---------- ---------------------------- 56.6/212.5 MB 1.7 MB/s eta 0:01:31\n",
      "     ---------- ---------------------------- 57.1/212.5 MB 1.7 MB/s eta 0:01:31\n",
      "     ---------- ---------------------------- 57.7/212.5 MB 1.8 MB/s eta 0:01:29\n",
      "     ---------- ---------------------------- 58.2/212.5 MB 1.8 MB/s eta 0:01:28\n",
      "     ---------- ---------------------------- 58.7/212.5 MB 1.8 MB/s eta 0:01:28\n",
      "     ---------- ---------------------------- 59.2/212.5 MB 1.8 MB/s eta 0:01:27\n",
      "     ---------- ---------------------------- 59.8/212.5 MB 1.8 MB/s eta 0:01:26\n",
      "     ----------- --------------------------- 60.3/212.5 MB 1.8 MB/s eta 0:01:25\n",
      "     ----------- --------------------------- 61.1/212.5 MB 1.8 MB/s eta 0:01:27\n",
      "     ----------- --------------------------- 61.3/212.5 MB 1.7 MB/s eta 0:01:27\n",
      "     ----------- --------------------------- 61.9/212.5 MB 1.7 MB/s eta 0:01:27\n",
      "     ----------- --------------------------- 62.1/212.5 MB 1.7 MB/s eta 0:01:27\n",
      "     ----------- --------------------------- 62.7/212.5 MB 1.7 MB/s eta 0:01:26\n",
      "     ----------- --------------------------- 63.2/212.5 MB 1.8 MB/s eta 0:01:24\n",
      "     ----------- --------------------------- 63.7/212.5 MB 1.8 MB/s eta 0:01:24\n",
      "     ----------- --------------------------- 64.2/212.5 MB 1.8 MB/s eta 0:01:24\n",
      "     ----------- --------------------------- 64.7/212.5 MB 1.8 MB/s eta 0:01:24\n",
      "     ----------- --------------------------- 65.0/212.5 MB 1.8 MB/s eta 0:01:23\n",
      "     ------------ -------------------------- 65.5/212.5 MB 1.8 MB/s eta 0:01:23\n",
      "     ------------ -------------------------- 66.1/212.5 MB 1.8 MB/s eta 0:01:23\n",
      "     ------------ -------------------------- 66.6/212.5 MB 1.7 MB/s eta 0:01:24\n",
      "     ------------ -------------------------- 66.8/212.5 MB 1.8 MB/s eta 0:01:23\n",
      "     ------------ -------------------------- 67.4/212.5 MB 1.8 MB/s eta 0:01:22\n",
      "     ------------ -------------------------- 67.9/212.5 MB 1.8 MB/s eta 0:01:22\n",
      "     ------------ -------------------------- 68.4/212.5 MB 1.8 MB/s eta 0:01:21\n",
      "     ------------ -------------------------- 68.9/212.5 MB 1.8 MB/s eta 0:01:21\n",
      "     ------------ -------------------------- 69.2/212.5 MB 1.8 MB/s eta 0:01:21\n",
      "     ------------ -------------------------- 69.7/212.5 MB 1.8 MB/s eta 0:01:20\n",
      "     ------------ -------------------------- 70.3/212.5 MB 1.8 MB/s eta 0:01:20\n",
      "     ------------ -------------------------- 70.8/212.5 MB 1.8 MB/s eta 0:01:20\n",
      "     ------------- ------------------------- 71.0/212.5 MB 1.8 MB/s eta 0:01:20\n",
      "     ------------- ------------------------- 71.6/212.5 MB 1.8 MB/s eta 0:01:19\n",
      "     ------------- ------------------------- 72.1/212.5 MB 1.8 MB/s eta 0:01:19\n",
      "     ------------- ------------------------- 72.6/212.5 MB 1.8 MB/s eta 0:01:19\n",
      "     ------------- ------------------------- 73.1/212.5 MB 1.8 MB/s eta 0:01:19\n",
      "     ------------- ------------------------- 73.4/212.5 MB 1.8 MB/s eta 0:01:18\n",
      "     ------------- ------------------------- 73.9/212.5 MB 1.8 MB/s eta 0:01:17\n",
      "     ------------- ------------------------- 74.4/212.5 MB 1.8 MB/s eta 0:01:17\n",
      "     ------------- ------------------------- 75.0/212.5 MB 1.8 MB/s eta 0:01:16\n",
      "     ------------- ------------------------- 75.5/212.5 MB 1.8 MB/s eta 0:01:16\n",
      "     ------------- ------------------------- 76.0/212.5 MB 1.8 MB/s eta 0:01:15\n",
      "     -------------- ------------------------ 76.3/212.5 MB 1.8 MB/s eta 0:01:15\n",
      "     -------------- ------------------------ 76.8/212.5 MB 1.8 MB/s eta 0:01:15\n",
      "     -------------- ------------------------ 77.3/212.5 MB 1.8 MB/s eta 0:01:14\n",
      "     -------------- ------------------------ 77.6/212.5 MB 1.8 MB/s eta 0:01:14\n",
      "     -------------- ------------------------ 77.9/212.5 MB 1.9 MB/s eta 0:01:13\n",
      "     -------------- ------------------------ 78.4/212.5 MB 1.9 MB/s eta 0:01:13\n",
      "     -------------- ------------------------ 78.9/212.5 MB 1.9 MB/s eta 0:01:12\n",
      "     -------------- ------------------------ 79.4/212.5 MB 1.9 MB/s eta 0:01:12\n",
      "     -------------- ------------------------ 80.0/212.5 MB 1.9 MB/s eta 0:01:10\n",
      "     -------------- ------------------------ 80.2/212.5 MB 1.9 MB/s eta 0:01:10\n",
      "     -------------- ------------------------ 80.7/212.5 MB 1.9 MB/s eta 0:01:10\n",
      "     -------------- ------------------------ 81.3/212.5 MB 1.9 MB/s eta 0:01:10\n",
      "     --------------- ----------------------- 81.8/212.5 MB 1.9 MB/s eta 0:01:08\n",
      "     --------------- ----------------------- 82.1/212.5 MB 1.9 MB/s eta 0:01:08\n",
      "     --------------- ----------------------- 82.6/212.5 MB 1.9 MB/s eta 0:01:08\n",
      "     --------------- ----------------------- 83.1/212.5 MB 1.9 MB/s eta 0:01:07\n",
      "     --------------- ----------------------- 83.6/212.5 MB 1.9 MB/s eta 0:01:07\n",
      "     --------------- ----------------------- 84.1/212.5 MB 1.9 MB/s eta 0:01:07\n",
      "     --------------- ----------------------- 84.7/212.5 MB 1.9 MB/s eta 0:01:07\n",
      "     --------------- ----------------------- 84.9/212.5 MB 1.9 MB/s eta 0:01:07\n",
      "     --------------- ----------------------- 85.7/212.5 MB 1.9 MB/s eta 0:01:06\n",
      "     --------------- ----------------------- 86.0/212.5 MB 1.9 MB/s eta 0:01:06\n",
      "     --------------- ----------------------- 86.5/212.5 MB 1.9 MB/s eta 0:01:05\n",
      "     --------------- ----------------------- 87.0/212.5 MB 2.0 MB/s eta 0:01:04\n",
      "     ---------------- ---------------------- 87.6/212.5 MB 2.0 MB/s eta 0:01:04\n",
      "     ---------------- ---------------------- 88.1/212.5 MB 2.0 MB/s eta 0:01:04\n",
      "     ---------------- ---------------------- 88.6/212.5 MB 2.0 MB/s eta 0:01:04\n",
      "     ---------------- ---------------------- 88.9/212.5 MB 2.0 MB/s eta 0:01:04\n",
      "     ---------------- ---------------------- 89.4/212.5 MB 2.0 MB/s eta 0:01:03\n",
      "     ---------------- ---------------------- 89.9/212.5 MB 2.0 MB/s eta 0:01:03\n",
      "     ---------------- ---------------------- 90.4/212.5 MB 2.0 MB/s eta 0:01:02\n",
      "     ---------------- ---------------------- 90.7/212.5 MB 2.0 MB/s eta 0:01:01\n",
      "     ---------------- ---------------------- 91.2/212.5 MB 2.0 MB/s eta 0:01:01\n",
      "     ---------------- ---------------------- 92.0/212.5 MB 2.0 MB/s eta 0:01:00\n",
      "     ----------------- --------------------- 92.8/212.5 MB 2.0 MB/s eta 0:01:00\n",
      "     ----------------- --------------------- 93.1/212.5 MB 2.0 MB/s eta 0:01:00\n",
      "     ----------------- --------------------- 94.4/212.5 MB 2.1 MB/s eta 0:00:58\n",
      "     ----------------- --------------------- 94.6/212.5 MB 2.1 MB/s eta 0:00:58\n",
      "     ----------------- --------------------- 94.9/212.5 MB 2.0 MB/s eta 0:00:58\n",
      "     ----------------- --------------------- 95.4/212.5 MB 2.1 MB/s eta 0:00:57\n",
      "     ----------------- --------------------- 96.5/212.5 MB 2.1 MB/s eta 0:00:57\n",
      "     ----------------- --------------------- 97.3/212.5 MB 2.1 MB/s eta 0:00:56\n",
      "     ----------------- --------------------- 97.8/212.5 MB 2.1 MB/s eta 0:00:55\n",
      "     ------------------ -------------------- 98.3/212.5 MB 2.1 MB/s eta 0:00:55\n",
      "     ------------------ -------------------- 99.1/212.5 MB 2.1 MB/s eta 0:00:54\n",
      "     ------------------ -------------------- 99.6/212.5 MB 2.1 MB/s eta 0:00:54\n",
      "     ----------------- -------------------- 100.4/212.5 MB 2.1 MB/s eta 0:00:53\n",
      "     ------------------ ------------------- 100.7/212.5 MB 2.1 MB/s eta 0:00:53\n",
      "     ------------------ ------------------- 101.4/212.5 MB 2.1 MB/s eta 0:00:52\n",
      "     ------------------ ------------------- 102.2/212.5 MB 2.2 MB/s eta 0:00:52\n",
      "     ------------------ ------------------- 102.5/212.5 MB 2.2 MB/s eta 0:00:51\n",
      "     ------------------ ------------------- 103.5/212.5 MB 2.2 MB/s eta 0:00:51\n",
      "     ------------------ ------------------- 104.1/212.5 MB 2.2 MB/s eta 0:00:50\n",
      "     ------------------ ------------------- 104.9/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------ ------------------- 104.9/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------ ------------------- 104.9/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------ ------------------- 104.9/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------ ------------------- 105.1/212.5 MB 2.2 MB/s eta 0:00:50\n",
      "     ------------------ ------------------- 105.9/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------- ------------------ 106.4/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------- ------------------ 107.0/212.5 MB 2.2 MB/s eta 0:00:49\n",
      "     ------------------- ------------------ 107.5/212.5 MB 2.2 MB/s eta 0:00:48\n",
      "     ------------------- ------------------ 108.0/212.5 MB 2.2 MB/s eta 0:00:48\n",
      "     ------------------- ------------------ 108.5/212.5 MB 2.2 MB/s eta 0:00:47\n",
      "     ------------------- ------------------ 109.3/212.5 MB 2.2 MB/s eta 0:00:47\n",
      "     ------------------- ------------------ 109.8/212.5 MB 2.2 MB/s eta 0:00:46\n",
      "     ------------------- ------------------ 110.4/212.5 MB 2.2 MB/s eta 0:00:46\n",
      "     ------------------- ------------------ 110.9/212.5 MB 2.2 MB/s eta 0:00:46\n",
      "     ------------------- ------------------ 111.4/212.5 MB 2.3 MB/s eta 0:00:45\n",
      "     ------------------- ------------------ 111.7/212.5 MB 2.2 MB/s eta 0:00:45\n",
      "     -------------------- ----------------- 111.9/212.5 MB 2.3 MB/s eta 0:00:45\n",
      "     -------------------- ----------------- 112.7/212.5 MB 2.3 MB/s eta 0:00:45\n",
      "     -------------------- ----------------- 113.2/212.5 MB 2.3 MB/s eta 0:00:44\n",
      "     -------------------- ----------------- 114.0/212.5 MB 2.3 MB/s eta 0:00:44\n",
      "     -------------------- ----------------- 114.3/212.5 MB 2.3 MB/s eta 0:00:43\n",
      "     -------------------- ----------------- 115.1/212.5 MB 2.3 MB/s eta 0:00:43\n",
      "     -------------------- ----------------- 115.6/212.5 MB 2.3 MB/s eta 0:00:43\n",
      "     -------------------- ----------------- 116.1/212.5 MB 2.3 MB/s eta 0:00:42\n",
      "     -------------------- ----------------- 116.9/212.5 MB 2.3 MB/s eta 0:00:42\n",
      "     -------------------- ----------------- 117.2/212.5 MB 2.3 MB/s eta 0:00:42\n",
      "     --------------------- ---------------- 117.4/212.5 MB 2.3 MB/s eta 0:00:41\n",
      "     --------------------- ---------------- 118.5/212.5 MB 2.3 MB/s eta 0:00:41\n",
      "     --------------------- ---------------- 119.0/212.5 MB 2.3 MB/s eta 0:00:40\n",
      "     --------------------- ---------------- 119.3/212.5 MB 2.3 MB/s eta 0:00:40\n",
      "     --------------------- ---------------- 120.1/212.5 MB 2.4 MB/s eta 0:00:40\n",
      "     --------------------- ---------------- 120.6/212.5 MB 2.4 MB/s eta 0:00:39\n",
      "     --------------------- ---------------- 121.4/212.5 MB 2.4 MB/s eta 0:00:39\n",
      "     --------------------- ---------------- 121.6/212.5 MB 2.4 MB/s eta 0:00:39\n",
      "     --------------------- ---------------- 122.4/212.5 MB 2.4 MB/s eta 0:00:38\n",
      "     --------------------- ---------------- 122.7/212.5 MB 2.4 MB/s eta 0:00:38\n",
      "     ---------------------- --------------- 123.2/212.5 MB 2.4 MB/s eta 0:00:38\n",
      "     ---------------------- --------------- 124.0/212.5 MB 2.4 MB/s eta 0:00:38\n",
      "     ---------------------- --------------- 124.8/212.5 MB 2.4 MB/s eta 0:00:37\n",
      "     ---------------------- --------------- 125.0/212.5 MB 2.4 MB/s eta 0:00:37\n",
      "     ---------------------- --------------- 126.1/212.5 MB 2.4 MB/s eta 0:00:36\n",
      "     ---------------------- --------------- 126.4/212.5 MB 2.4 MB/s eta 0:00:36\n",
      "     ---------------------- --------------- 126.6/212.5 MB 2.4 MB/s eta 0:00:36\n",
      "     ---------------------- --------------- 127.4/212.5 MB 2.4 MB/s eta 0:00:35\n",
      "     ---------------------- --------------- 128.2/212.5 MB 2.5 MB/s eta 0:00:35\n",
      "     ----------------------- -------------- 129.0/212.5 MB 2.4 MB/s eta 0:00:35\n",
      "     ----------------------- -------------- 129.2/212.5 MB 2.4 MB/s eta 0:00:35\n",
      "     ----------------------- -------------- 130.8/212.5 MB 2.5 MB/s eta 0:00:34\n",
      "     ----------------------- -------------- 131.6/212.5 MB 2.5 MB/s eta 0:00:33\n",
      "     ----------------------- -------------- 132.9/212.5 MB 2.5 MB/s eta 0:00:32\n",
      "     ----------------------- -------------- 133.4/212.5 MB 2.5 MB/s eta 0:00:32\n",
      "     ----------------------- -------------- 134.0/212.5 MB 2.5 MB/s eta 0:00:32\n",
      "     ------------------------ ------------- 135.0/212.5 MB 2.5 MB/s eta 0:00:31\n",
      "     ------------------------ ------------- 135.5/212.5 MB 2.5 MB/s eta 0:00:31\n",
      "     ------------------------ ------------- 136.1/212.5 MB 2.5 MB/s eta 0:00:31\n",
      "     ------------------------ ------------- 136.8/212.5 MB 2.5 MB/s eta 0:00:30\n",
      "     ------------------------ ------------- 137.9/212.5 MB 2.6 MB/s eta 0:00:30\n",
      "     ------------------------ ------------- 138.9/212.5 MB 2.6 MB/s eta 0:00:29\n",
      "     ------------------------ ------------- 139.7/212.5 MB 2.6 MB/s eta 0:00:29\n",
      "     ------------------------- ------------ 140.2/212.5 MB 2.6 MB/s eta 0:00:28\n",
      "     ------------------------- ------------ 141.3/212.5 MB 2.6 MB/s eta 0:00:28\n",
      "     ------------------------- ------------ 142.6/212.5 MB 2.6 MB/s eta 0:00:27\n",
      "     ------------------------- ------------ 143.7/212.5 MB 2.6 MB/s eta 0:00:26\n",
      "     ------------------------- ------------ 144.4/212.5 MB 2.7 MB/s eta 0:00:26\n",
      "     -------------------------- ----------- 145.5/212.5 MB 2.7 MB/s eta 0:00:26\n",
      "     -------------------------- ----------- 145.8/212.5 MB 2.7 MB/s eta 0:00:25\n",
      "     -------------------------- ----------- 146.8/212.5 MB 2.7 MB/s eta 0:00:25\n",
      "     -------------------------- ----------- 147.6/212.5 MB 2.7 MB/s eta 0:00:24\n",
      "     -------------------------- ----------- 148.1/212.5 MB 2.7 MB/s eta 0:00:24\n",
      "     -------------------------- ----------- 149.7/212.5 MB 2.7 MB/s eta 0:00:23\n",
      "     -------------------------- ----------- 150.7/212.5 MB 2.8 MB/s eta 0:00:23\n",
      "     --------------------------- ---------- 151.5/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 152.6/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 153.4/212.5 MB 2.8 MB/s eta 0:00:22\n",
      "     --------------------------- ---------- 154.1/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 155.2/212.5 MB 2.8 MB/s eta 0:00:21\n",
      "     --------------------------- ---------- 156.0/212.5 MB 2.8 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 156.8/212.5 MB 2.9 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 157.8/212.5 MB 2.9 MB/s eta 0:00:20\n",
      "     ---------------------------- --------- 158.6/212.5 MB 2.9 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 159.6/212.5 MB 2.9 MB/s eta 0:00:19\n",
      "     ---------------------------- --------- 160.4/212.5 MB 2.9 MB/s eta 0:00:18\n",
      "     ---------------------------- --------- 161.2/212.5 MB 2.9 MB/s eta 0:00:18\n",
      "     ----------------------------- -------- 162.3/212.5 MB 2.9 MB/s eta 0:00:18\n",
      "     ----------------------------- -------- 163.1/212.5 MB 3.0 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 163.6/212.5 MB 3.0 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 164.4/212.5 MB 3.0 MB/s eta 0:00:17\n",
      "     ----------------------------- -------- 165.2/212.5 MB 3.0 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 166.2/212.5 MB 3.0 MB/s eta 0:00:16\n",
      "     ----------------------------- -------- 167.2/212.5 MB 3.0 MB/s eta 0:00:16\n",
      "     ------------------------------ ------- 168.3/212.5 MB 3.0 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 169.1/212.5 MB 3.1 MB/s eta 0:00:15\n",
      "     ------------------------------ ------- 170.1/212.5 MB 3.1 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 170.9/212.5 MB 3.1 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 171.7/212.5 MB 3.1 MB/s eta 0:00:14\n",
      "     ------------------------------ ------- 172.8/212.5 MB 3.1 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 173.5/212.5 MB 3.1 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 174.6/212.5 MB 3.1 MB/s eta 0:00:13\n",
      "     ------------------------------- ------ 175.4/212.5 MB 3.2 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 176.2/212.5 MB 3.2 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 177.2/212.5 MB 3.2 MB/s eta 0:00:12\n",
      "     ------------------------------- ------ 178.0/212.5 MB 3.2 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 179.0/212.5 MB 3.2 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 179.8/212.5 MB 3.2 MB/s eta 0:00:11\n",
      "     -------------------------------- ----- 180.6/212.5 MB 3.2 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 181.7/212.5 MB 3.3 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 182.5/212.5 MB 3.3 MB/s eta 0:00:10\n",
      "     -------------------------------- ----- 183.2/212.5 MB 3.3 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 184.3/212.5 MB 3.3 MB/s eta 0:00:09\n",
      "     --------------------------------- ---- 185.1/212.5 MB 3.3 MB/s eta 0:00:09\n",
      "     --------------------------------- ---- 186.1/212.5 MB 3.3 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 186.9/212.5 MB 3.3 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 188.0/212.5 MB 3.4 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 188.7/212.5 MB 3.4 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 189.5/212.5 MB 3.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 190.6/212.5 MB 3.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 191.4/212.5 MB 3.4 MB/s eta 0:00:07\n",
      "     ---------------------------------- --- 192.2/212.5 MB 3.4 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 193.2/212.5 MB 3.4 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 194.0/212.5 MB 3.4 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 195.0/212.5 MB 3.5 MB/s eta 0:00:06\n",
      "     ----------------------------------- -- 195.8/212.5 MB 3.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 196.6/212.5 MB 3.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 197.7/212.5 MB 3.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 198.4/212.5 MB 3.5 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 199.2/212.5 MB 3.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 200.3/212.5 MB 3.5 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 201.1/212.5 MB 3.5 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 201.9/212.5 MB 3.5 MB/s eta 0:00:04\n",
      "     ------------------------------------ - 202.9/212.5 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 203.7/212.5 MB 3.5 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 204.7/212.5 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 205.5/212.5 MB 3.6 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 206.3/212.5 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------------------------  207.4/212.5 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------------------------  208.1/212.5 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------------------------  209.2/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  209.7/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  209.7/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  211.0/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.3/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------  212.3/212.5 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 212.5/212.5 MB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.47.1)\n",
      "Collecting transformers\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a9/b6/5257d04ae327b44db31f15cce39e6020cc986333c715660b1315a9724d82/transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Requirement already satisfied: trl in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (0.17.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from trl) (3.5.0)\n",
      "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from trl) (13.7.1)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets>=3.0.0->trl) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->trl) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->trl) (2.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=3.0.0->trl) (1.11.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Installing collected packages: sympy, torch, transformers\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "Successfully installed sympy-1.14.0 torch-2.7.0 transformers-4.51.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in /home/vipuser/anaconda3/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: packaging in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (3.3.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: trl in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: rich in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (4.51.3)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (21.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (1.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (0.31.1)\n",
      "Requirement already satisfied: pyyaml in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (6.0)\n",
      "Requirement already satisfied: psutil in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (5.8.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (2.7.0)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.3.1)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (1.3.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (21.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2021.10.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.85)\n",
      "Requirement already satisfied: jinja2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: networkx in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.26.2)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.3.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.80)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate>=0.34.0->trl) (58.0.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.0->trl) (1.2.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers>=4.46.0->trl) (2021.8.3)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in /home/vipuser/anaconda3/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /home/vipuser/anaconda3/lib/python3.9/site-packages (4.51.3)\n",
      "Requirement already satisfied: trl in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: fsspec in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from triton==3.3.0->torch) (58.0.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: rich in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: psutil in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (5.8.0)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec->torch) (3.11.18)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.20.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (6.4.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (21.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple huggingface_hub\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple datasets\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple trl\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade torch transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b4dc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.31.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (21.0)\n",
      "Requirement already satisfied: requests in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61c5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748e1841-74f5-46ca-87a4-4cb81ec6ae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118\n",
      "import done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(torch.__version__)\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86332daa-8fa7-43c7-8aaf-b8ba4eb7dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face API \n"
     ]
    }
   ],
   "source": [
    "# log in\n",
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Hugging Face\n",
    "hf_token = \"\"\n",
    "\n",
    "# Hugging Face\n",
    "login(token=hf_token)\n",
    "\n",
    "# \n",
    "api = HfApi()\n",
    "token = HfFolder.get_token()\n",
    "if token:\n",
    "    print(\"Hugging Face API \")\n",
    "else:\n",
    "    print(\"Hugging Face API \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce73c82-4c37-44bb-a6c7-5fd63bfbd427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00393442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 11.99 GB\n",
      ": 0.00 GB\n",
      ": 11.99 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#  CUDA \n",
    "if torch.cuda.is_available():\n",
    "    # \n",
    "    device = torch.cuda.current_device()\n",
    "    # \n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    # \n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    # \n",
    "    free_memory = total_memory - allocated_memory\n",
    "    \n",
    "    print(f\": {total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\": {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\": {free_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\" CUDA \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10516e5a-9180-455d-a6ba-c6c89b9b4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def format_instruction_example(example):\n",
    "    question = example[\"Question\"]          # instruction\n",
    "    complex_cot = example[\"Complex_CoT\"]    # input\n",
    "    response = example[\"Response\"]          # output\n",
    "    \n",
    "    # Complex_CoT\n",
    "    if complex_cot and complex_cot.strip() != \"\":\n",
    "        text = f\"### Question:\\n{question}\\n\\n### Complex_CoT:\\n{complex_cot}\\n\\n### Response:\\n{response}\"\n",
    "    else:\n",
    "        text = f\"### Question:\\n{question}\\n\\n### Response:\\n{response}\"\n",
    "    \n",
    "    # \n",
    "    encoded = tokenizer(text, truncation=True, max_length=512)\n",
    "    \n",
    "    # \n",
    "    encoded[\"text\"] = text\n",
    "    \n",
    "    # token_ids\n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac1f6011-c6ba-4aa2-ab18-6067e66667cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model and dataset load done\n"
     ]
    }
   ],
   "source": [
    "# # \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "# model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-1.4b\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# \n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", name=\"en\", split=\"train\")\n",
    "\n",
    "train_dataset = dataset.select(range(10))\n",
    "\n",
    "# \n",
    "formatted_dataset = train_dataset.map(format_instruction_example)\n",
    "\n",
    "print('model and dataset load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c059723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "for param in model.parameters():\n",
    "    if param.device.type == 'cuda':\n",
    "        print(\" GPU \")\n",
    "        break\n",
    "else:\n",
    "    print(\" GPU  CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9d170f-5d81-4c26-9801-fe58112f204c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 :\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Question:\n",
      "\n",
      "### Q...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 :\n",
      "The patient is stable. She's got a 5-cm stab wound to the left upper chest. She's got a pneumothorax, and she's got a low blood pressure. She's got a heart rate of 110, and her blood pressure is 90.\n",
      "\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 :\n",
      "### Question:\n",
      "A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 :\n",
      "Okay, so we have a 45-year-old man with a history of alcohol use, who has been abstinent for the past 10 years. He suddenly starts showing some pretty specific symptoms: dysarthria, shuffling gait, an...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 :\n",
      "I think that's a great point. I think that's a great point. I think that's a great point. I think that's a great point. I think that's a great point. I think that's a great point. I think that's a gre...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6 :\n",
      "I'm not sure what you mean by \"pustular psoriasis.\" I've seen this before, but I don't recall it being generalized. I think it's more likely to be localized to the skin, like a rash. \n",
      "\n",
      "Okay, so pustul...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7 :\n",
      "I think the most likely diagnosis is SCFE.\n",
      "\n",
      "The child is 70 kg, and the symptoms are consistent with SCFE.\n",
      "\n",
      "The child is a toddler, and the symptoms are consistent with SCFE.\n",
      "\n",
      "The child is a 2-year-ol...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8 :\n",
      "### Question:\n",
      "What is the maximum amount of a drug that can be given to a patient?\n",
      "\n",
      "### Complex_CoT:\n",
      "The maximum amount of a drug that can be given to a patient is the amount that will give the maximu...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9 :\n",
      "Im glad you brought this up because its a very interesting case. Im not sure if youve ever heard of Klinefelter syndrome, but its a condition that affects males with an extra X chromosome.\n",
      "\n",
      "Kline...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      " 10 :\n",
      "### Question:\n",
      "In a case where a child with pneumonia is diagnosed, and their chest X-ray indicates a pattern consistent with a common bacterial infection, what is the most likely causative organism ba...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "# \n",
    "def generate_text(question, max_length=200):\n",
    "    \"\"\"200 tokens\"\"\"\n",
    "    # \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=min(len(inputs.input_ids[0]) + max_length, 2048),  # \n",
    "            temperature=0.0,      # \n",
    "            num_beams=1,          # \n",
    "            do_sample=False       # \n",
    "        )\n",
    "    \n",
    "    # \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(question):].strip()  # \n",
    "\n",
    "# 10\n",
    "for i, example in enumerate(train_dataset):\n",
    "    question = example[\"Question\"]\n",
    "    \n",
    "    # Complex_CoT\n",
    "    if example[\"Complex_CoT\"] and example[\"Complex_CoT\"].strip() != \"\":\n",
    "        prompt = f\"### Question:\\n{question}\\n\\n### Complex_CoT:\\n{example['Complex_CoT']}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Question:\\n{question}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # \n",
    "    generated_response = generate_text(prompt)\n",
    "    \n",
    "    # \n",
    "    print(f\" {i+1} :\")\n",
    "    print(generated_response[:200] + (\"...\" if len(generated_response) > 200 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c82c9c2-7b7d-4c29-9334-d3baea532324",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncating train dataset: 100%|| 10/10 [00:00<00:00, 1780.79 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m\n\u001b[0;32m      5\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m      7\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m     30\u001b[0m         model,\n\u001b[0;32m     31\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39mformatted_dataset,\n\u001b[0;32m     32\u001b[0m         args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     33\u001b[0m     )\n\u001b[1;32m---> 35\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3742\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\trl\\trainer\\sft_trainer.py:654\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    653\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 654\u001b[0m (loss, outputs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[0;32m    655\u001b[0m     model, inputs, return_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch\n\u001b[0;32m    656\u001b[0m )\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:814\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\utils\\operations.py:802\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:803\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(GPT_NEOX_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    758\u001b[0m \u001b[38;5;129m@replace_return_docstrings\u001b[39m(output_type\u001b[38;5;241m=\u001b[39mCausalLMOutputWithPast, config_class\u001b[38;5;241m=\u001b[39m_CONFIG_FOR_DOC)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[KwargsForCausalLM],\n\u001b[0;32m    774\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n\u001b[0;32m    775\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m        Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;124;03m    >>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpt_neox(\n\u001b[0;32m    804\u001b[0m         input_ids,\n\u001b[0;32m    805\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    806\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    807\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    808\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    809\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    810\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    811\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    812\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    813\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    815\u001b[0m     )\n\u001b[0;32m    817\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[0;32m    818\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[0;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:572\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    560\u001b[0m         layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    561\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m         position_embeddings,\n\u001b[0;32m    570\u001b[0m     )\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 572\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m layer(\n\u001b[0;32m    573\u001b[0m         hidden_states,\n\u001b[0;32m    574\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m    575\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m    576\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m    577\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    578\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    579\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    580\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    581\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[0;32m    583\u001b[0m     )\n\u001b[0;32m    584\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:272\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_dropout(attn_output)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_parallel_residual:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# pseudocode:\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# x = x + attn(ln1(x)) + mlp(ln2(x))\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states))\n\u001b[0;32m    273\u001b[0m     mlp_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_mlp_dropout(mlp_output)\n\u001b[0;32m    274\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m mlp_output \u001b[38;5;241m+\u001b[39m attn_output \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\gpt_neox\\modeling_gpt_neox.py:61\u001b[0m, in \u001b[0;36mGPTNeoXMLP.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m---> 61\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_h_to_4h(hidden_states)\n\u001b[0;32m     62\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m     63\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_4h_to_h(hidden_states)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# origin train method\n",
    "\n",
    "try:\n",
    "\n",
    "    # training_args = SFTConfig(output_dir=\"/tmp\")\n",
    "    #  SFT \n",
    "    training_args = SFTConfig(\n",
    "    # \n",
    "    output_dir=\"/tmp\",\n",
    "    # \n",
    "    num_train_epochs=3,\n",
    "    #  GPU \n",
    "    per_device_train_batch_size=8,\n",
    "    # \n",
    "    gradient_accumulation_steps=4,\n",
    "    # \n",
    "    learning_rate=2e-5,\n",
    "    # \n",
    "    weight_decay=0.01,\n",
    "    # \n",
    "    warmup_steps=500,\n",
    "    #  AdamW\n",
    "    optim=\"adamw_torch\",\n",
    "    # \n",
    "    save_steps=10_000,\n",
    "    # \n",
    "    logging_steps=100,\n",
    "    #  fp16 \n",
    "    fp16=True,\n",
    ")\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=formatted_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\": {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80317475-46fa-4ca1-8edf-d4b27f8456b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": SFTConfig.__init__() got an unexpected keyword argument 'use_eval_mode'\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "try:\n",
    "    #  SFT  -  SFTConfig \n",
    "    training_args = SFTConfig(\n",
    "        output_dir=\"/tmp\",\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.0,  # \n",
    "        warmup_steps=0,\n",
    "        optim=\"adamw_torch\",\n",
    "\n",
    "        logging_steps=1,\n",
    "        fp16=False,\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=formatted_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\": {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32bf5d9d-0ab3-48a3-9f05-a361427e783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fine_tuned_pythia\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "save_path = r'fine_tuned_pythia_TestVersion'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\" {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b04e6-15af-46cf-90b7-1a387de7d9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c45603-27b3-4d15-9965-a90c8a95c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:\n",
      "GPU: 0.00 MB\n",
      "GPU: 0.00 MB\n",
      " \n",
      " \n",
      "\n",
      "GPU:\n",
      "GPU: 0.00 MB\n",
      "GPU: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# GPU\n",
    "def print_gpu_memory():\n",
    "    \"\"\"GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"GPU\")\n",
    "\n",
    "# GPU\n",
    "print(\"GPU:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# \n",
    "try:\n",
    "    # CPU\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        model.to('cpu')\n",
    "        del model\n",
    "        print(\" CPU\")\n",
    "    else:\n",
    "        print(\" \")\n",
    "    \n",
    "    # \n",
    "    if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "        print(\" \")\n",
    "    else:\n",
    "        print(\" \")\n",
    "    \n",
    "    # GPU\n",
    "    if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "        del train_dataset\n",
    "    if 'val_dataset' in locals() or 'val_dataset' in globals():\n",
    "        del val_dataset\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # \n",
    "    gc.collect()\n",
    "    \n",
    "    # GPU\n",
    "    print(\"\\nGPU:\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\": {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d4f02a-e6ad-4d90-afcc-fc29c056915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tokenizer = AutoTokenizer.from_pretrained('fine_tuned_pythia_TestVersion')\n",
    "model = AutoModelForCausalLM.from_pretrained('fine_tuned_pythia_TestVersion')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.eval()  # \n",
    "\n",
    "# \n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", name=\"en\", split=\"train\")\n",
    "train_dataset = dataset.select(range(10))\n",
    "\n",
    "# \n",
    "def generate_text(question, max_length=200):\n",
    "    \"\"\"200 tokens\"\"\"\n",
    "    # \n",
    "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=min(len(inputs.input_ids[0]) + max_length, 2048),  # \n",
    "            temperature=0.0,      # \n",
    "            num_beams=1,          # \n",
    "            do_sample=False       # \n",
    "        )\n",
    "\n",
    "    # \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(question):].strip()  # \n",
    "\n",
    "# 10\n",
    "for i, example in enumerate(train_dataset):\n",
    "    question = example[\"Question\"]\n",
    "\n",
    "    # Complex_CoT\n",
    "    if example[\"Complex_CoT\"] and example[\"Complex_CoT\"].strip() != \"\":\n",
    "        prompt = f\"### Question:\\n{question}\\n\\n### Complex_CoT:\\n{example['Complex_CoT']}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Question:\\n{question}\\n\\n### Response:\\n\"\n",
    "\n",
    "    # \n",
    "    generated_response = generate_text(prompt)\n",
    "\n",
    "    # \n",
    "    print(f\" {i + 1} :\")\n",
    "    print(generated_response[:200] + (\"...\" if len(generated_response) > 200 else \"\"))\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c37d6b-f2f4-484a-a674-76e83debf8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    save_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# \n",
    "prompt = \"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# \n",
    "generate_config = {\n",
    "    \"max_length\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "# \n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, streamer=streamer, **generate_config)\n",
    "\n",
    "# streamer\n",
    "if not streamer:\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4bead75-02e4-443d-a223-2b482e5b6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# \n",
    "# target_start = \"\"\n",
    "target_start = \"\"\n",
    "target_start_ids = tokenizer(target_start, return_tensors=\"pt\").input_ids.squeeze().tolist()\n",
    "\n",
    "# \n",
    "n_tokens = 15          # \n",
    "num_steps = 150         # GCG\n",
    "k = 128                # top-k\n",
    "B = 64                 # \n",
    "sample_size = n_tokens      # token\n",
    "max_generation_length = len(target_start_ids) + 20  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efe3cb02-785c-430a-944c-20eb36f0d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, target_start_ids):\n",
    "    \"\"\"\"\"\"\n",
    "    target_tensor = torch.tensor(target_start_ids, device=logits.device).long()\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    target_len = len(target_start_ids)\n",
    "    \n",
    "    # \n",
    "    if seq_len < target_len:\n",
    "        return torch.max(logits) * 1e6\n",
    "    \n",
    "    # logits\n",
    "    loss = F.cross_entropy(\n",
    "        logits[:, :target_len, :].reshape(-1, vocab_size), \n",
    "        target_tensor.repeat(batch_size)\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46f128-20d7-4d00-87ec-efd2df7c3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(model, input_ids, position, vocab_size, target_start_ids):\n",
    "    \"\"\"tokenone-hot\"\"\"\n",
    "    embed_layer = model.get_input_embeddings()\n",
    "    actual_vocab_size = embed_layer.weight.shape[0]\n",
    "    \n",
    "    #  one-hot \n",
    "    one_hot = torch.zeros(actual_vocab_size, device=device, dtype=embed_layer.weight.dtype)\n",
    "    one_hot.requires_grad_()\n",
    "    \n",
    "    input_embeds = embed_layer(input_ids.unsqueeze(0)).detach().clone()\n",
    "    input_embeds[0, position] = torch.matmul(one_hot.unsqueeze(0), embed_layer.weight)\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        logits = model(inputs_embeds=input_embeds).logits\n",
    "        \n",
    "        # \n",
    "        if logits.shape[1] < len(target_start_ids):\n",
    "            loss = torch.max(logits) * 1e6\n",
    "        else:\n",
    "            # positiontarget_start_ids\n",
    "            if position < len(target_start_ids):\n",
    "                relevant_logits = logits[0, position]\n",
    "                target = torch.tensor([target_start_ids[position]], device=device)\n",
    "                loss = F.cross_entropy(relevant_logits.unsqueeze(0), target)\n",
    "            else:\n",
    "                # position\n",
    "                loss = torch.tensor(1e-6, device=device, requires_grad=True)\n",
    "            \n",
    "        loss.backward()\n",
    "    \n",
    "    # None\n",
    "    if one_hot.grad is None:\n",
    "        return torch.zeros_like(one_hot, device=device)\n",
    "    \n",
    "    return one_hot.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb80f24-bc5f-4784-9e75-50b429ab46d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def gcg_algorithm():\n",
    "    # n_tokenstoken\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    initial_prompt = torch.randint(0, vocab_size, (n_tokens,), device=device).tolist()\n",
    "    current_prompt = initial_prompt.copy()\n",
    "    step_losses = []\n",
    "    \n",
    "    print(f\"[INIT] Initial Prompt: {tokenizer.decode(current_prompt)}\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # tensor\n",
    "        input_ids = torch.tensor(current_prompt, device=device).long()\n",
    "        grads = []  # [seq_len, vocab_size]\n",
    "        \n",
    "        # \n",
    "        for pos in range(n_tokens):\n",
    "            grad = calculate_gradient(model, input_ids, pos, vocab_size, target_start_ids)\n",
    "            grads.append(grad.cpu().numpy())  # CPU\n",
    "        \n",
    "        # \n",
    "        candidates = []\n",
    "        for _ in range(B):\n",
    "            new_prompt = current_prompt.copy()\n",
    "            pos = np.random.randint(n_tokens)  # \n",
    "            \n",
    "            # ktoken\n",
    "            top_k_indices = np.argsort(grads[pos])[:k]  # \n",
    "            new_token = np.random.choice(top_k_indices)  # token\n",
    "            new_prompt[pos] = new_token\n",
    "            candidates.append(new_prompt)\n",
    "        \n",
    "        # \n",
    "        best_loss = float('inf')\n",
    "        best_candidate = current_prompt\n",
    "        for cand in candidates:\n",
    "            input_ids_cand = torch.tensor(cand, device=device).long().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids_cand).logits\n",
    "                loss = compute_loss(logits, target_start_ids)\n",
    "            \n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_candidate = cand\n",
    "        \n",
    "        # \n",
    "        current_prompt = best_candidate\n",
    "        step_losses.append(best_loss)\n",
    "        print(f\"[STEP {step + 1}] Loss: {best_loss:.4f}, Prompt: {tokenizer.decode(current_prompt)}\")\n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(step_losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"GCG Optimization Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return current_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83d093d3-9122-4569-8403-0550dd6a8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniprompt_algorithm():\n",
    "    global n_tokens\n",
    "    global num_steps\n",
    "    running_min = 0\n",
    "    running_max = 100000\n",
    "    best = None\n",
    "    while True:\n",
    "        z = gcg_algorithm()\n",
    "        input_ids = torch.tensor(z, device=device).long().unsqueeze(0)\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # \n",
    "            generated_output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_beams=50,\n",
    "                no_repeat_ngram_size=2,\n",
    "                early_stopping=False,  # \n",
    "                temperature=0.2\n",
    "            )\n",
    "            generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "            print(f\"Generated text: {generated_text} :end\")\n",
    "            print(f\"with token of {n_tokens}\")\n",
    "            \n",
    "            # \n",
    "            if generated_text.strip() == target_start.strip():\n",
    "                running_max = n_tokens\n",
    "                n_tokens = max(1, n_tokens - 1)  # 1token\n",
    "                best = z\n",
    "                num_steps = max(50, num_steps - 10)  # \n",
    "                print(f\" Matched at {n_tokens} tokens!\")\n",
    "            else:\n",
    "                running_min = n_tokens\n",
    "                n_tokens = min(n_tokens + 5, running_max)  # 5token\n",
    "                num_steps = min(500, num_steps + 50)       # \n",
    "                print(f\" Not matched, increase to {n_tokens} tokens\")\n",
    "\n",
    "        if n_tokens >= running_max:\n",
    "            print(\"Reached maximum token limit, stopping...\")\n",
    "            break\n",
    "        if n_tokens <= running_min:\n",
    "            print(\"Reached minimum token limit, stopping...\")\n",
    "            break\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78ea3229-9a07-4304-a617-c3318e61acb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT] Initial Prompt: Cr Irvine overlooking input Works prosperutures disemb\n",
      "[STEP 1] Loss: 9.5667, Prompt:  Irvine overlooking input Works prosperutures disemb\n",
      "[STEP 2] Loss: 9.2116, Prompt: holm Irvine overlooking input Works prosperutures disemb\n",
      "[STEP 3] Loss: 8.9380, Prompt: holm Irvine overlooking input Works prosperutures disemb\n",
      "[STEP 4] Loss: 8.2564, Prompt: holm Irvine overlooking input Works prosperutures disemb\n",
      "[STEP 5] Loss: 7.8705, Prompt: holm overlooking input Works prosperutures disemb\n",
      "[STEP 6] Loss: 7.4140, Prompt: holm overlooking input Works prosperutures disemb\n",
      "[STEP 7] Loss: 7.0509, Prompt: holm overlooking input Works prosperutures disemb\n",
      "[STEP 8] Loss: 7.0509, Prompt: holm overlooking input Works prosperutures disemb\n",
      "[STEP 9] Loss: 7.0509, Prompt: holm overlooking Works prosperutures disemb\n",
      "[STEP 10] Loss: 7.0509, Prompt: holm overlooking Works prosperutures disemb\n",
      "[STEP 11] Loss: 6.8404, Prompt: holm overlooking Works prosperutures disemb\n",
      "[STEP 12] Loss: 6.6022, Prompt: holmIVE overlooking Works prosperutures disemb\n",
      "[STEP 13] Loss: 6.5674, Prompt: holmigel overlooking Works prosperutures disemb\n",
      "[STEP 14] Loss: 6.1941, Prompt: holmigel overlooking Works prosperutures disemb\n",
      "[STEP 15] Loss: 6.1941, Prompt: holmigel overlooking Works prosperutures disemb\n",
      "[STEP 16] Loss: 6.1941, Prompt: holmigel overlooking prosperutures disemb\n",
      "[STEP 17] Loss: 6.1941, Prompt: holmigel overlooking prosperutures disemb\n",
      "[STEP 18] Loss: 6.1941, Prompt: holmigel overlooking prosper disemb\n",
      "[STEP 19] Loss: 6.1941, Prompt: holmigel overlooking prosper disemb\n",
      "[STEP 20] Loss: 6.1941, Prompt: holmigel overlooking prosper disemb\n",
      "[STEP 21] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 22] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 23] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 24] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 25] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 26] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 27] Loss: 6.1941, Prompt: holmigel prosper disemb\n",
      "[STEP 28] Loss: 6.1208, Prompt: holm prosper disemb\n",
      "[STEP 29] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 30] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 31] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 32] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 33] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 34] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 35] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 36] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 37] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 38] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 39] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 40] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 41] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 42] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 43] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 44] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 45] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 46] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 47] Loss: 6.1208, Prompt: holm prosper\n",
      "[STEP 48] Loss: 6.1208, Prompt: holm\n",
      "[STEP 49] Loss: 6.1208, Prompt: holm\n",
      "[STEP 50] Loss: 6.1208, Prompt: holm\n",
      "[STEP 51] Loss: 6.1208, Prompt: holm\n",
      "[STEP 52] Loss: 6.1208, Prompt: holm\n",
      "[STEP 53] Loss: 6.1208, Prompt: holm\n",
      "[STEP 54] Loss: 5.8823, Prompt: \n",
      "[STEP 55] Loss: 5.8823, Prompt: \n",
      "[STEP 56] Loss: 5.8823, Prompt: \n",
      "[STEP 57] Loss: 5.8823, Prompt: \n",
      "[STEP 58] Loss: 5.8823, Prompt: \n",
      "[STEP 59] Loss: 5.8823, Prompt: \n",
      "[STEP 60] Loss: 5.8823, Prompt: \n",
      "[STEP 61] Loss: 5.8823, Prompt: \n",
      "[STEP 62] Loss: 5.8823, Prompt: \n",
      "[STEP 63] Loss: 5.8823, Prompt: \n",
      "[STEP 64] Loss: 5.8823, Prompt: \n",
      "[STEP 65] Loss: 5.8823, Prompt: \n",
      "[STEP 66] Loss: 5.5214, Prompt: \n",
      "[STEP 67] Loss: 5.4793, Prompt: ays\n",
      "[STEP 68] Loss: 5.4793, Prompt: ays\n",
      "[STEP 69] Loss: 5.4793, Prompt: ays\n",
      "[STEP 70] Loss: 5.4793, Prompt: ays\n",
      "[STEP 71] Loss: 5.4445, Prompt: oralays\n",
      "[STEP 72] Loss: 5.4445, Prompt: oralays\n",
      "[STEP 73] Loss: 5.4445, Prompt: oralays\n",
      "[STEP 74] Loss: 5.3827, Prompt: eralays\n",
      "[STEP 75] Loss: 5.3827, Prompt: eralays\n",
      "[STEP 76] Loss: 5.3827, Prompt: eralays\n",
      "[STEP 77] Loss: 5.3827, Prompt: eralays\n",
      "[STEP 78] Loss: 5.3212, Prompt: erale\n",
      "[STEP 79] Loss: 5.3212, Prompt: erale\n",
      "[STEP 80] Loss: 5.3212, Prompt: erale\n",
      "[STEP 81] Loss: 5.3212, Prompt: erale\n",
      "[STEP 82] Loss: 5.3212, Prompt: erale\n",
      "[STEP 83] Loss: 5.3212, Prompt: erale\n",
      "[STEP 84] Loss: 5.3212, Prompt: erale\n",
      "[STEP 85] Loss: 5.3212, Prompt: erale\n",
      "[STEP 86] Loss: 5.3212, Prompt: erale\n",
      "[STEP 87] Loss: 5.3212, Prompt: erale\n",
      "[STEP 88] Loss: 5.3212, Prompt: erale\n",
      "[STEP 89] Loss: 5.3212, Prompt: erale\n",
      "[STEP 90] Loss: 5.3212, Prompt: erale\n",
      "[STEP 91] Loss: 5.3212, Prompt: erale\n",
      "[STEP 92] Loss: 5.3212, Prompt: erale\n",
      "[STEP 93] Loss: 5.3212, Prompt: erale\n",
      "[STEP 94] Loss: 5.3212, Prompt: erale\n",
      "[STEP 95] Loss: 5.3212, Prompt: erale\n",
      "[STEP 96] Loss: 5.3212, Prompt: erale\n",
      "[STEP 97] Loss: 5.3212, Prompt: erale\n",
      "[STEP 98] Loss: 5.3212, Prompt: erale\n",
      "[STEP 99] Loss: 5.3212, Prompt: erale\n",
      "[STEP 100] Loss: 5.3212, Prompt: erale\n",
      "[STEP 101] Loss: 5.3212, Prompt: erale\n",
      "[STEP 102] Loss: 5.3212, Prompt: erale\n",
      "[STEP 103] Loss: 5.3212, Prompt: erale\n",
      "[STEP 104] Loss: 5.3212, Prompt: erale\n",
      "[STEP 105] Loss: 5.3212, Prompt: erale\n",
      "[STEP 106] Loss: 5.3212, Prompt: erale\n",
      "[STEP 107] Loss: 5.3212, Prompt: erale\n",
      "[STEP 108] Loss: 5.3212, Prompt: erale\n",
      "[STEP 109] Loss: 5.3212, Prompt: erale\n",
      "[STEP 110] Loss: 5.3212, Prompt: erale\n",
      "[STEP 111] Loss: 5.3146, Prompt: eralivated\n",
      "[STEP 112] Loss: 5.3146, Prompt: eralivated\n",
      "[STEP 113] Loss: 5.3146, Prompt: eralivated\n",
      "[STEP 114] Loss: 5.3146, Prompt: eralivated\n",
      "[STEP 115] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 116] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 117] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 118] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 119] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 120] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 121] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 122] Loss: 5.2739, Prompt: uteivated\n",
      "[STEP 123] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 124] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 125] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 126] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 127] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 128] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 129] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 130] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 131] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 132] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 133] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 134] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 135] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 136] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 137] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 138] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 139] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 140] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 141] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 142] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 143] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 144] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 145] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 146] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 147] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 148] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 149] Loss: 5.2315, Prompt: uteivated\n",
      "[STEP 150] Loss: 5.2315, Prompt: uteivated\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: uteivated\n",
      "\n",
      " :end\n",
      "with token of 15\n",
      " Not matched, increase to 20 tokens\n",
      "[INIT] Initial Prompt:  Croatia-tests PINposiointhidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 1] Loss: 12.2398, Prompt:  Croatia-tests\n",
      "\n",
      "PINposiointhidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 2] Loss: 11.3409, Prompt: -tests\n",
      "\n",
      "PINposiointhidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 3] Loss: 10.7644, Prompt: \n",
      "\n",
      "PINposiointhidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 4] Loss: 10.0427, Prompt: \n",
      "\n",
      "posiointhidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 5] Loss: 9.8705, Prompt: \n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 6] Loss: 9.5381, Prompt: \n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 7] Loss: 9.2056, Prompt: \n",
      "\n",
      "xicinthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 8] Loss: 8.6762, Prompt: <|endoftext|>\n",
      "\n",
      "xicinthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 9] Loss: 8.5814, Prompt: <|endoftext|>\n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 10] Loss: 8.5814, Prompt: <|endoftext|>\n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 11] Loss: 8.5716, Prompt: <|endoftext|>\n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 12] Loss: 8.3257, Prompt: \n",
      "\n",
      "inthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 13] Loss: 7.8277, Prompt: isinthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 14] Loss: 7.8277, Prompt: isinthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 15] Loss: 7.5914, Prompt: isinthidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 16] Loss: 7.5914, Prompt: isidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 17] Loss: 7.5914, Prompt: isidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 18] Loss: 7.5914, Prompt: isidinest rendez ninth  paraBTN_BACK\n",
      "[STEP 19] Loss: 7.5914, Prompt: isidine rendez ninth  paraBTN_BACK\n",
      "[STEP 20] Loss: 7.5524, Prompt: alledidine rendez ninth  paraBTN_BACK\n",
      "[STEP 21] Loss: 7.5524, Prompt: alledidine rendez  paraBTN_BACK\n",
      "[STEP 22] Loss: 7.5524, Prompt: alledidine rendez  paraBTN_BACK\n",
      "[STEP 23] Loss: 7.4314, Prompt: alled idine rendez  paraBTN_BACK\n",
      "[STEP 24] Loss: 7.4314, Prompt: alled idine rendez BTN_BACK\n",
      "[STEP 25] Loss: 7.2638, Prompt: alledidine rendez BTN_BACK\n",
      "[STEP 26] Loss: 7.2638, Prompt: alledidine rendez BTN_BACK\n",
      "[STEP 27] Loss: 7.2638, Prompt: alledidine rendez BTN_BACK\n",
      "[STEP 28] Loss: 7.1527, Prompt: alledidine rendez BTN_BACK\n",
      "[STEP 29] Loss: 7.1527, Prompt: alled rendez BTN_BACK\n",
      "[STEP 30] Loss: 7.1527, Prompt: alled rendezBTN_BACK\n",
      "[STEP 31] Loss: 7.1527, Prompt: alled rendezBTN_BACK\n",
      "[STEP 32] Loss: 7.1527, Prompt: alled rendezBTN\n",
      "[STEP 33] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 34] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 35] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 36] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 37] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 38] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 39] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 40] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 41] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 42] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 43] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 44] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 45] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 46] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 47] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 48] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 49] Loss: 7.1527, Prompt: alled rendez\n",
      "[STEP 50] Loss: 7.1527, Prompt: alled\n",
      "[STEP 51] Loss: 7.1527, Prompt: alled\n",
      "[STEP 52] Loss: 7.1527, Prompt: alled\n",
      "[STEP 53] Loss: 7.1527, Prompt: alled\n",
      "[STEP 54] Loss: 7.1527, Prompt: alled\n",
      "[STEP 55] Loss: 7.1527, Prompt: alled\n",
      "[STEP 56] Loss: 7.1527, Prompt: alled\n",
      "[STEP 57] Loss: 7.1527, Prompt: alled\n",
      "[STEP 58] Loss: 7.1527, Prompt: alled\n",
      "[STEP 59] Loss: 7.1527, Prompt: alled\n",
      "[STEP 60] Loss: 7.1527, Prompt: alled\n",
      "[STEP 61] Loss: 7.1527, Prompt: alled\n",
      "[STEP 62] Loss: 7.1527, Prompt: alled\n",
      "[STEP 63] Loss: 7.1527, Prompt: alled\n",
      "[STEP 64] Loss: 7.1527, Prompt: alled\n",
      "[STEP 65] Loss: 7.1527, Prompt: alled\n",
      "[STEP 66] Loss: 7.1527, Prompt: alled\n",
      "[STEP 67] Loss: 7.1527, Prompt: alled\n",
      "[STEP 68] Loss: 7.1527, Prompt: alled\n",
      "[STEP 69] Loss: 7.1527, Prompt: alled\n",
      "[STEP 70] Loss: 7.1527, Prompt: alled\n",
      "[STEP 71] Loss: 7.1527, Prompt: alled\n",
      "[STEP 72] Loss: 7.1527, Prompt: alled\n",
      "[STEP 73] Loss: 7.1527, Prompt: alled\n",
      "[STEP 74] Loss: 7.1527, Prompt: alled\n",
      "[STEP 75] Loss: 7.1527, Prompt: alled\n",
      "[STEP 76] Loss: 7.1527, Prompt: alled\n",
      "[STEP 77] Loss: 7.1527, Prompt: alled\n",
      "[STEP 78] Loss: 7.1527, Prompt: alled\n",
      "[STEP 79] Loss: 7.1527, Prompt: alled\n",
      "[STEP 80] Loss: 7.1527, Prompt: alled\n",
      "[STEP 81] Loss: 7.1527, Prompt: alled\n",
      "[STEP 82] Loss: 7.1527, Prompt: alled\n",
      "[STEP 83] Loss: 7.1527, Prompt: alled\n",
      "[STEP 84] Loss: 7.1527, Prompt: alled\n",
      "[STEP 85] Loss: 7.1527, Prompt: alled\n",
      "[STEP 86] Loss: 7.1527, Prompt: alled\n",
      "[STEP 87] Loss: 7.1527, Prompt: alled\n",
      "[STEP 88] Loss: 7.1527, Prompt: alled\n",
      "[STEP 89] Loss: 7.1527, Prompt: alled\n",
      "[STEP 90] Loss: 7.1527, Prompt: alled\n",
      "[STEP 91] Loss: 7.1527, Prompt: alled\n",
      "[STEP 92] Loss: 7.1527, Prompt: alled\n",
      "[STEP 93] Loss: 7.1527, Prompt: alled\n",
      "[STEP 94] Loss: 7.1527, Prompt: alled\n",
      "[STEP 95] Loss: 7.1527, Prompt: alled\n",
      "[STEP 96] Loss: 7.1527, Prompt: alled\n",
      "[STEP 97] Loss: 7.1527, Prompt: alled\n",
      "[STEP 98] Loss: 7.1527, Prompt: alled\n",
      "[STEP 99] Loss: 7.1527, Prompt: alled\n",
      "[STEP 100] Loss: 7.1412, Prompt: llealled\n",
      "[STEP 101] Loss: 7.1412, Prompt: llealled\n",
      "[STEP 102] Loss: 7.0219, Prompt: lleICI\n",
      "[STEP 103] Loss: 7.0219, Prompt: lleICI\n",
      "[STEP 104] Loss: 7.0219, Prompt: lleICI\n",
      "[STEP 105] Loss: 7.0219, Prompt: lleICI\n",
      "[STEP 106] Loss: 7.0219, Prompt: lleICI\n",
      "[STEP 107] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 108] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 109] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 110] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 111] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 112] Loss: 6.8676, Prompt: lleICI\n",
      "[STEP 113] Loss: 6.7410, Prompt: ICI\n",
      "[STEP 114] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 115] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 116] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 117] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 118] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 119] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 120] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 121] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 122] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 123] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 124] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 125] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 126] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 127] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 128] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 129] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 130] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 131] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 132] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 133] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 134] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 135] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 136] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 137] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 138] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 139] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 140] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 141] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 142] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 143] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 144] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 145] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 146] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 147] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 148] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 149] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 150] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 151] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 152] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 153] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 154] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 155] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 156] Loss: 6.5230, Prompt: ICI\n",
      "[STEP 157] Loss: 6.4146, Prompt: \n",
      "[STEP 158] Loss: 6.4146, Prompt: \n",
      "[STEP 159] Loss: 6.4146, Prompt: \n",
      "[STEP 160] Loss: 6.4146, Prompt: \n",
      "[STEP 161] Loss: 6.4146, Prompt: \n",
      "[STEP 162] Loss: 6.4146, Prompt: \n",
      "[STEP 163] Loss: 6.4146, Prompt: \n",
      "[STEP 164] Loss: 6.4146, Prompt: \n",
      "[STEP 165] Loss: 6.4146, Prompt: \n",
      "[STEP 166] Loss: 6.4146, Prompt: \n",
      "[STEP 167] Loss: 6.4146, Prompt: \n",
      "[STEP 168] Loss: 6.4146, Prompt: \n",
      "[STEP 169] Loss: 6.4146, Prompt: \n",
      "[STEP 170] Loss: 6.4146, Prompt: \n",
      "[STEP 171] Loss: 6.4146, Prompt: \n",
      "[STEP 172] Loss: 6.4146, Prompt: \n",
      "[STEP 173] Loss: 6.4146, Prompt: \n",
      "[STEP 174] Loss: 6.4146, Prompt: \n",
      "[STEP 175] Loss: 6.4146, Prompt: \n",
      "[STEP 176] Loss: 6.4146, Prompt: \n",
      "[STEP 177] Loss: 6.4146, Prompt: \n",
      "[STEP 178] Loss: 6.4146, Prompt: \n",
      "[STEP 179] Loss: 6.4146, Prompt: \n",
      "[STEP 180] Loss: 6.4146, Prompt: \n",
      "[STEP 181] Loss: 6.4146, Prompt: \n",
      "[STEP 182] Loss: 6.4146, Prompt: \n",
      "[STEP 183] Loss: 6.4146, Prompt: \n",
      "[STEP 184] Loss: 6.4146, Prompt: \n",
      "[STEP 185] Loss: 6.4146, Prompt: \n",
      "[STEP 186] Loss: 6.4146, Prompt: \n",
      "[STEP 187] Loss: 6.4146, Prompt: \n",
      "[STEP 188] Loss: 6.4146, Prompt: \n",
      "[STEP 189] Loss: 6.4146, Prompt: \n",
      "[STEP 190] Loss: 6.4146, Prompt: \n",
      "[STEP 191] Loss: 6.4146, Prompt: \n",
      "[STEP 192] Loss: 6.4146, Prompt: \n",
      "[STEP 193] Loss: 6.4146, Prompt: \n",
      "[STEP 194] Loss: 6.4146, Prompt: \n",
      "[STEP 195] Loss: 6.4146, Prompt: \n",
      "[STEP 196] Loss: 6.4146, Prompt: \n",
      "[STEP 197] Loss: 6.4146, Prompt: \n",
      "[STEP 198] Loss: 6.4146, Prompt: \n",
      "[STEP 199] Loss: 6.4146, Prompt: \n",
      "[STEP 200] Loss: 6.3523, Prompt: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: \n",
      "\n",
      " :end\n",
      "with token of 20\n",
      " Not matched, increase to 25 tokens\n",
      "[INIT] Initial Prompt:   tornlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 1] Loss: 12.0705, Prompt:   tornlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 2] Loss: 11.0269, Prompt:  MethodBeatlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 3] Loss: 10.4242, Prompt:  MethodBeatlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 4] Loss: 9.8017, Prompt: MethodBeatlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 5] Loss: 9.3148, Prompt: inementlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 6] Loss: 9.3148, Prompt: inementlowestsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 7] Loss: 8.5475, Prompt: inementapingsj\"))) kindly Finder.from librarian units.tkCompose  edio.environmentn---- Thy \n",
      "[STEP 8] Loss: 8.5475, Prompt: inementapingsj\"))) kindly Finder.from librarian units.tk  edio.environmentn---- Thy \n",
      "[STEP 9] Loss: 8.5475, Prompt: inementapingsj\"))) Finder.from librarian units.tk  edio.environmentn---- Thy \n",
      "[STEP 10] Loss: 8.5475, Prompt: inementapingsj\"))) Finder.from librarian units.tk .environmentn---- Thy \n",
      "[STEP 11] Loss: 8.5475, Prompt: inementapingsj\"))) Finder.from librarian units .environmentn---- Thy \n",
      "[STEP 12] Loss: 8.5475, Prompt: inementapingsj\"))) Finder.from librarian .environmentn---- Thy \n",
      "[STEP 13] Loss: 8.5475, Prompt: inementapingsj\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 14] Loss: 7.9220, Prompt: inementapingsj\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 15] Loss: 7.9220, Prompt: inementapingsj\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 16] Loss: 7.7667, Prompt: ificeapingsj\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 17] Loss: 7.5192, Prompt: ificeapingled\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 18] Loss: 7.5192, Prompt: ificeapingled\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 19] Loss: 7.4460, Prompt: ificeapingly\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 20] Loss: 7.2572, Prompt: ificeapingons\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 21] Loss: 7.2333, Prompt: ificeons\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 22] Loss: 7.2333, Prompt: ificeons\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 23] Loss: 7.1982, Prompt: ificeons\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 24] Loss: 7.1982, Prompt: ificeons\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 25] Loss: 6.8730, Prompt: ifice\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 26] Loss: 6.7426, Prompt: agne\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 27] Loss: 6.7426, Prompt: agne\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 28] Loss: 6.7426, Prompt: agne\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 29] Loss: 6.4713, Prompt: agne\n",
      "\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 30] Loss: 6.4713, Prompt: agne\n",
      "\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 31] Loss: 6.4713, Prompt: agne\n",
      "\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 32] Loss: 6.4713, Prompt: agne\n",
      "\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 33] Loss: 6.4713, Prompt: agne\n",
      "\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 34] Loss: 6.4080, Prompt: agne\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 35] Loss: 6.3764, Prompt: angent\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 36] Loss: 6.3764, Prompt: angent\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 37] Loss: 6.3764, Prompt: angent\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 38] Loss: 6.3764, Prompt: angent\"))) Finder librarian .environmentn---- Thy \n",
      "[STEP 39] Loss: 6.3764, Prompt: angent\"))) librarian .environmentn---- Thy \n",
      "[STEP 40] Loss: 6.3764, Prompt: angent\"))) librarian .environmentn---- Thy \n",
      "[STEP 41] Loss: 6.3764, Prompt: angent\"))) librarian n---- Thy \n",
      "[STEP 42] Loss: 6.3764, Prompt: angent\"))) librarian n---- Thy \n",
      "[STEP 43] Loss: 6.3259, Prompt: \"))) librarian n---- Thy \n",
      "[STEP 44] Loss: 6.3259, Prompt: \"))) n---- Thy \n",
      "[STEP 45] Loss: 6.3259, Prompt: \"))) n---- Thy \n",
      "[STEP 46] Loss: 6.3259, Prompt: \"))) n---- Thy \n",
      "[STEP 47] Loss: 6.3259, Prompt: \"))) n---- Thy \n",
      "[STEP 48] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 49] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 50] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 51] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 52] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 53] Loss: 6.3259, Prompt: \")))n---- Thy \n",
      "[STEP 54] Loss: 6.3259, Prompt: \")))n---- \n",
      "[STEP 55] Loss: 6.3259, Prompt: \")))n---- \n",
      "[STEP 56] Loss: 6.3259, Prompt: \")))n---- \n",
      "[STEP 57] Loss: 6.3259, Prompt: \")))n---- \n",
      "[STEP 58] Loss: 6.1180, Prompt: \")))n---- \n",
      "[STEP 59] Loss: 6.1180, Prompt: \")))n---- \n",
      "[STEP 60] Loss: 6.1180, Prompt: \")))n---- \n",
      "[STEP 61] Loss: 6.0006, Prompt: ometry\")))n---- \n",
      "[STEP 62] Loss: 6.0006, Prompt: ometry\")))n---- \n",
      "[STEP 63] Loss: 6.0006, Prompt: ometry\")))n---- \n",
      "[STEP 64] Loss: 6.0006, Prompt: ometry\")))n---- \n",
      "[STEP 65] Loss: 6.0006, Prompt: ometry\")))n---- \n",
      "[STEP 66] Loss: 5.9638, Prompt: etre\")))n---- \n",
      "[STEP 67] Loss: 5.9638, Prompt: etre\")))n---- \n",
      "[STEP 68] Loss: 5.9638, Prompt: etre\")))n---- \n",
      "[STEP 69] Loss: 5.8848, Prompt: ckill\")))n---- \n",
      "[STEP 70] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 71] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 72] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 73] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 74] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 75] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 76] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 77] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 78] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 79] Loss: 5.8159, Prompt: EK\")))n---- \n",
      "[STEP 80] Loss: 5.8159, Prompt: EK\")))n----\n",
      "[STEP 81] Loss: 5.8159, Prompt: EKn----\n",
      "[STEP 82] Loss: 5.8159, Prompt: EKn----\n",
      "[STEP 83] Loss: 5.8159, Prompt: EKn----\n",
      "[STEP 84] Loss: 5.8159, Prompt: EKn----\n",
      "[STEP 85] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 86] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 87] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 88] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 89] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 90] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 91] Loss: 5.8159, Prompt: EKn\n",
      "[STEP 92] Loss: 5.8159, Prompt: EK\n",
      "[STEP 93] Loss: 5.8159, Prompt: EK\n",
      "[STEP 94] Loss: 5.8159, Prompt: EK\n",
      "[STEP 95] Loss: 5.8159, Prompt: EK\n",
      "[STEP 96] Loss: 5.4792, Prompt: EK\n",
      "[STEP 97] Loss: 5.4792, Prompt: EK\n",
      "[STEP 98] Loss: 5.4792, Prompt: EK\n",
      "[STEP 99] Loss: 5.4792, Prompt: EK\n",
      "[STEP 100] Loss: 5.4792, Prompt: EK\n",
      "[STEP 101] Loss: 5.4792, Prompt: EK\n",
      "[STEP 102] Loss: 5.4792, Prompt: EK\n",
      "[STEP 103] Loss: 5.4792, Prompt: EK\n",
      "[STEP 104] Loss: 5.4792, Prompt: EK\n",
      "[STEP 105] Loss: 5.4792, Prompt: EK\n",
      "[STEP 106] Loss: 5.4792, Prompt: EK\n",
      "[STEP 107] Loss: 5.4792, Prompt: EK\n",
      "[STEP 108] Loss: 5.4792, Prompt: EK\n",
      "[STEP 109] Loss: 5.4792, Prompt: EK\n",
      "[STEP 110] Loss: 5.4792, Prompt: EK\n",
      "[STEP 111] Loss: 5.4792, Prompt: EK\n",
      "[STEP 112] Loss: 5.4792, Prompt: EK\n",
      "[STEP 113] Loss: 5.4792, Prompt: EK\n",
      "[STEP 114] Loss: 5.4792, Prompt: EK\n",
      "[STEP 115] Loss: 5.4792, Prompt: EK\n",
      "[STEP 116] Loss: 5.4792, Prompt: EK\n",
      "[STEP 117] Loss: 5.4792, Prompt: EK\n",
      "[STEP 118] Loss: 5.4792, Prompt: EK\n",
      "[STEP 119] Loss: 5.4792, Prompt: EK\n",
      "[STEP 120] Loss: 5.4792, Prompt: EK\n",
      "[STEP 121] Loss: 5.4792, Prompt: EK\n",
      "[STEP 122] Loss: 5.4792, Prompt: EK\n",
      "[STEP 123] Loss: 5.4792, Prompt: EK\n",
      "[STEP 124] Loss: 5.4792, Prompt: EK\n",
      "[STEP 125] Loss: 5.4792, Prompt: EK\n",
      "[STEP 126] Loss: 5.4792, Prompt: EK\n",
      "[STEP 127] Loss: 5.4792, Prompt: EK\n",
      "[STEP 128] Loss: 5.4792, Prompt: EK\n",
      "[STEP 129] Loss: 5.4792, Prompt: EK\n",
      "[STEP 130] Loss: 5.4792, Prompt: EK\n",
      "[STEP 131] Loss: 5.4792, Prompt: EK\n",
      "[STEP 132] Loss: 5.4792, Prompt: EK\n",
      "[STEP 133] Loss: 5.4792, Prompt: EK\n",
      "[STEP 134] Loss: 5.4792, Prompt: EK\n",
      "[STEP 135] Loss: 5.4792, Prompt: EK\n",
      "[STEP 136] Loss: 5.4792, Prompt: EK\n",
      "[STEP 137] Loss: 5.4792, Prompt: EK\n",
      "[STEP 138] Loss: 5.4792, Prompt: EK\n",
      "[STEP 139] Loss: 5.4792, Prompt: EK\n",
      "[STEP 140] Loss: 5.4792, Prompt: EK\n",
      "[STEP 141] Loss: 5.4792, Prompt: EK\n",
      "[STEP 142] Loss: 5.4792, Prompt: EK\n",
      "[STEP 143] Loss: 5.4792, Prompt: EK\n",
      "[STEP 144] Loss: 5.4792, Prompt: EK\n",
      "[STEP 145] Loss: 5.4792, Prompt: EK\n",
      "[STEP 146] Loss: 5.4792, Prompt: EK\n",
      "[STEP 147] Loss: 5.4792, Prompt: EK\n",
      "[STEP 148] Loss: 5.4792, Prompt: EK\n",
      "[STEP 149] Loss: 5.4792, Prompt: EK\n",
      "[STEP 150] Loss: 5.4792, Prompt: EK\n",
      "[STEP 151] Loss: 5.4792, Prompt: EK\n",
      "[STEP 152] Loss: 5.4792, Prompt: EK\n",
      "[STEP 153] Loss: 5.4792, Prompt: EK\n",
      "[STEP 154] Loss: 5.4792, Prompt: EK\n",
      "[STEP 155] Loss: 5.4792, Prompt: EK\n",
      "[STEP 156] Loss: 5.4792, Prompt: EK\n",
      "[STEP 157] Loss: 5.4792, Prompt: EK\n",
      "[STEP 158] Loss: 5.4792, Prompt: EK\n",
      "[STEP 159] Loss: 5.4792, Prompt: EK\n",
      "[STEP 160] Loss: 5.4792, Prompt: EK\n",
      "[STEP 161] Loss: 5.4792, Prompt: EK\n",
      "[STEP 162] Loss: 5.4792, Prompt: EK\n",
      "[STEP 163] Loss: 5.4792, Prompt: EK\n",
      "[STEP 164] Loss: 5.4792, Prompt: EK\n",
      "[STEP 165] Loss: 5.4792, Prompt: EK\n",
      "[STEP 166] Loss: 5.4792, Prompt: EK\n",
      "[STEP 167] Loss: 5.4792, Prompt: EK\n",
      "[STEP 168] Loss: 5.4792, Prompt: EK\n",
      "[STEP 169] Loss: 5.4792, Prompt: EK\n",
      "[STEP 170] Loss: 5.4792, Prompt: EK\n",
      "[STEP 171] Loss: 5.4792, Prompt: EK\n",
      "[STEP 172] Loss: 5.4792, Prompt: EK\n",
      "[STEP 173] Loss: 5.4792, Prompt: EK\n",
      "[STEP 174] Loss: 5.4792, Prompt: EK\n",
      "[STEP 175] Loss: 5.4792, Prompt: EK\n",
      "[STEP 176] Loss: 5.4792, Prompt: EK\n",
      "[STEP 177] Loss: 5.4792, Prompt: EK\n",
      "[STEP 178] Loss: 5.4792, Prompt: EK\n",
      "[STEP 179] Loss: 5.4792, Prompt: EK\n",
      "[STEP 180] Loss: 5.4792, Prompt: EK\n",
      "[STEP 181] Loss: 5.4792, Prompt: EK\n",
      "[STEP 182] Loss: 5.4792, Prompt: EK\n",
      "[STEP 183] Loss: 5.4792, Prompt: EK\n",
      "[STEP 184] Loss: 5.4792, Prompt: EK\n",
      "[STEP 185] Loss: 5.4792, Prompt: EK\n",
      "[STEP 186] Loss: 5.4792, Prompt: EK\n",
      "[STEP 187] Loss: 5.4792, Prompt: EK\n",
      "[STEP 188] Loss: 5.4792, Prompt: EK\n",
      "[STEP 189] Loss: 5.4792, Prompt: EK\n",
      "[STEP 190] Loss: 5.4792, Prompt: EK\n",
      "[STEP 191] Loss: 5.4792, Prompt: EK\n",
      "[STEP 192] Loss: 5.4792, Prompt: EK\n",
      "[STEP 193] Loss: 5.4792, Prompt: EK\n",
      "[STEP 194] Loss: 5.4792, Prompt: EK\n",
      "[STEP 195] Loss: 5.4792, Prompt: EK\n",
      "[STEP 196] Loss: 5.4792, Prompt: EK\n",
      "[STEP 197] Loss: 5.4792, Prompt: EK\n",
      "[STEP 198] Loss: 5.4792, Prompt: EK\n",
      "[STEP 199] Loss: 5.4792, Prompt: EK\n",
      "[STEP 200] Loss: 5.4792, Prompt: EK\n",
      "[STEP 201] Loss: 5.4792, Prompt: EK\n",
      "[STEP 202] Loss: 5.4792, Prompt: EK\n",
      "[STEP 203] Loss: 5.4792, Prompt: EK\n",
      "[STEP 204] Loss: 5.4792, Prompt: EK\n",
      "[STEP 205] Loss: 5.4792, Prompt: EK\n",
      "[STEP 206] Loss: 5.4792, Prompt: EK\n",
      "[STEP 207] Loss: 5.4792, Prompt: EK\n",
      "[STEP 208] Loss: 5.4792, Prompt: EK\n",
      "[STEP 209] Loss: 5.4792, Prompt: EK\n",
      "[STEP 210] Loss: 5.4792, Prompt: EK\n",
      "[STEP 211] Loss: 5.4792, Prompt: EK\n",
      "[STEP 212] Loss: 5.4792, Prompt: EK\n",
      "[STEP 213] Loss: 5.4792, Prompt: EK\n",
      "[STEP 214] Loss: 5.4792, Prompt: EK\n",
      "[STEP 215] Loss: 5.4792, Prompt: EK\n",
      "[STEP 216] Loss: 5.4792, Prompt: EK\n",
      "[STEP 217] Loss: 5.4792, Prompt: EK\n",
      "[STEP 218] Loss: 5.4792, Prompt: EK\n",
      "[STEP 219] Loss: 5.4792, Prompt: EK\n",
      "[STEP 220] Loss: 5.4792, Prompt: EK\n",
      "[STEP 221] Loss: 5.4792, Prompt: EK\n",
      "[STEP 222] Loss: 5.4792, Prompt: EK\n",
      "[STEP 223] Loss: 5.4792, Prompt: EK\n",
      "[STEP 224] Loss: 5.4792, Prompt: EK\n",
      "[STEP 225] Loss: 5.4792, Prompt: EK\n",
      "[STEP 226] Loss: 5.4792, Prompt: EK\n",
      "[STEP 227] Loss: 5.4792, Prompt: EK\n",
      "[STEP 228] Loss: 5.4792, Prompt: EK\n",
      "[STEP 229] Loss: 5.4792, Prompt: EK\n",
      "[STEP 230] Loss: 5.4792, Prompt: EK\n",
      "[STEP 231] Loss: 5.4792, Prompt: EK\n",
      "[STEP 232] Loss: 5.4792, Prompt: EK\n",
      "[STEP 233] Loss: 5.4792, Prompt: EK\n",
      "[STEP 234] Loss: 5.4792, Prompt: EK\n",
      "[STEP 235] Loss: 5.4792, Prompt: EK\n",
      "[STEP 236] Loss: 5.4792, Prompt: EK\n",
      "[STEP 237] Loss: 5.4792, Prompt: EK\n",
      "[STEP 238] Loss: 5.4792, Prompt: EK\n",
      "[STEP 239] Loss: 5.4792, Prompt: EK\n",
      "[STEP 240] Loss: 5.4792, Prompt: EK\n",
      "[STEP 241] Loss: 5.4792, Prompt: EK\n",
      "[STEP 242] Loss: 5.4792, Prompt: EK\n",
      "[STEP 243] Loss: 5.4792, Prompt: EK\n",
      "[STEP 244] Loss: 5.4792, Prompt: EK\n",
      "[STEP 245] Loss: 5.4792, Prompt: EK\n",
      "[STEP 246] Loss: 5.4792, Prompt: EK\n",
      "[STEP 247] Loss: 5.4792, Prompt: EK\n",
      "[STEP 248] Loss: 5.4792, Prompt: EK\n",
      "[STEP 249] Loss: 5.4792, Prompt: EK\n",
      "[STEP 250] Loss: 5.4792, Prompt: EK\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: EK\n",
      "\n",
      "EK :end\n",
      "with token of 25\n",
      " Not matched, increase to 30 tokens\n",
      "[INIT] Initial Prompt:  meticulously AbramsYep polluted Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 1] Loss: 11.1746, Prompt:  AbramsYep polluted Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 2] Loss: 10.3352, Prompt:  AbramsYep polluted Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 3] Loss: 10.0360, Prompt:  AbramsYepches Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 4] Loss: 9.7077, Prompt:  AbramsYepches Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 5] Loss: 8.8813, Prompt: elfareYepches Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 6] Loss: 8.7813, Prompt: elfareYeptheidches Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 7] Loss: 8.4774, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS\twriter huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 8] Loss: 8.4774, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 9] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS huis CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 10] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 11] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 12] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 13] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertex nervousBob unwind UTH Galactic\n",
      "[STEP 14] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertexBob unwind UTH Galactic\n",
      "[STEP 15] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinateng glVertexBob unwind UTH Galactic\n",
      "[STEP 16] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertexBob unwind UTH Galactic\n",
      "[STEP 17] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertexBob unwind UTH Galactic\n",
      "[STEP 18] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertexBob UTH Galactic\n",
      "[STEP 19] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertex UTH Galactic\n",
      "[STEP 20] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertex UTH Galactic\n",
      "[STEP 21] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari.Client_THIS CLLocationCoordinate glVertex UTH Galactic\n",
      "[STEP 22] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari_THIS CLLocationCoordinate glVertex UTH Galactic\n",
      "[STEP 23] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari_THIS CLLocationCoordinate glVertexUTH Galactic\n",
      "[STEP 24] Loss: 8.2723, Prompt: elfareYeptheidermal Stevenson Change salari_THIS CLLocationCoordinate glVertexUTH Galactic\n",
      "[STEP 25] Loss: 7.9475, Prompt: elfareYeptheidermal Stevenson Change salari_THIS CLLocationCoordinate glVertexUTH Galactic\n",
      "[STEP 26] Loss: 7.9475, Prompt: elfareYeptheidermal Stevenson Change salari CLLocationCoordinate glVertexUTH Galactic\n",
      "[STEP 27] Loss: 7.9475, Prompt: elfareYeptheidermal Stevenson salari CLLocationCoordinate glVertexUTH Galactic\n",
      "[STEP 28] Loss: 7.9475, Prompt: elfareYeptheidermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 29] Loss: 7.7413, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 30] Loss: 7.7413, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 31] Loss: 7.6250, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 32] Loss: 7.6250, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 33] Loss: 7.6250, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 34] Loss: 7.6250, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 35] Loss: 7.6250, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 36] Loss: 7.6232, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 37] Loss: 7.6232, Prompt: elfareYepermal Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 38] Loss: 7.3304, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 39] Loss: 7.3304, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 40] Loss: 7.3304, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 41] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 42] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 43] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 44] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 45] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 46] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 47] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 48] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 49] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate glVertex Galactic\n",
      "[STEP 50] Loss: 7.3261, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 51] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 52] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 53] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 54] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 55] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 56] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 57] Loss: 7.1634, Prompt: elfareYep Stevenson salari CLLocationCoordinate Galactic\n",
      "[STEP 58] Loss: 7.1634, Prompt: elfareYep Stevenson CLLocationCoordinate Galactic\n",
      "[STEP 59] Loss: 7.1634, Prompt: elfareYep Stevenson CLLocationCoordinate Galactic\n",
      "[STEP 60] Loss: 7.1634, Prompt: elfareYep CLLocationCoordinate Galactic\n",
      "[STEP 61] Loss: 7.1634, Prompt: elfareYep CLLocationCoordinate Galactic\n",
      "[STEP 62] Loss: 7.1634, Prompt: elfareYep CLLocationCoordinate Galactic\n",
      "[STEP 63] Loss: 7.1634, Prompt: elfareYep CLLocationCoordinate Galactic\n",
      "[STEP 64] Loss: 7.1634, Prompt: elfareYep CLLocationCoordinate Galactic\n",
      "[STEP 65] Loss: 7.1634, Prompt: elfareYep Galactic\n",
      "[STEP 66] Loss: 7.1634, Prompt: elfareYep Galactic\n",
      "[STEP 67] Loss: 7.1634, Prompt: elfareYep Galactic\n",
      "[STEP 68] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 69] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 70] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 71] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 72] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 73] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 74] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 75] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 76] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 77] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 78] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 79] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 80] Loss: 7.0583, Prompt: elfareYep Galactic\n",
      "[STEP 81] Loss: 7.0278, Prompt: elfareYepachment Galactic\n",
      "[STEP 82] Loss: 7.0278, Prompt: elfareYepachment Galactic\n",
      "[STEP 83] Loss: 7.0135, Prompt: Yepachment Galactic\n",
      "[STEP 84] Loss: 7.0135, Prompt: Yepachment Galactic\n",
      "[STEP 85] Loss: 7.0135, Prompt: Yepachment Galactic\n",
      "[STEP 86] Loss: 7.0135, Prompt: Yepachment Galactic\n",
      "[STEP 87] Loss: 6.7376, Prompt: achment Galactic\n",
      "[STEP 88] Loss: 6.7376, Prompt: achment Galactic\n",
      "[STEP 89] Loss: 6.7376, Prompt: achment Galactic\n",
      "[STEP 90] Loss: 6.7376, Prompt: achment Galactic\n",
      "[STEP 91] Loss: 6.7376, Prompt: achment Galactic\n",
      "[STEP 92] Loss: 6.5381, Prompt: achment Galactic\n",
      "[STEP 93] Loss: 6.4277, Prompt: achment Galactic\n",
      "[STEP 94] Loss: 6.1091, Prompt: osphate Galactic\n",
      "[STEP 95] Loss: 6.1091, Prompt: osphate Galactic\n",
      "[STEP 96] Loss: 6.1091, Prompt: osphate Galactic\n",
      "[STEP 97] Loss: 6.1091, Prompt: osphate Galactic\n",
      "[STEP 98] Loss: 6.1091, Prompt: osphate Galactic\n",
      "[STEP 99] Loss: 6.0512, Prompt: osphate Galactic\n",
      "[STEP 100] Loss: 6.0512, Prompt: osphate Galactic\n",
      "[STEP 101] Loss: 6.0512, Prompt: osphate Galactic\n",
      "[STEP 102] Loss: 5.8616, Prompt: osphate Galactic\n",
      "[STEP 103] Loss: 5.8616, Prompt: osphate Galactic\n",
      "[STEP 104] Loss: 5.8616, Prompt: osphate Galactic\n",
      "[STEP 105] Loss: 5.8073, Prompt: osphate Galactic\n",
      "[STEP 106] Loss: 5.8073, Prompt: osphate Galactic\n",
      "[STEP 107] Loss: 5.8073, Prompt: osphate Galactic\n",
      "[STEP 108] Loss: 5.8073, Prompt: osphate Galactic\n",
      "[STEP 109] Loss: 5.7348, Prompt: osphate Galactic\n",
      "[STEP 110] Loss: 5.4375, Prompt: osphate Galactic\n",
      "[STEP 111] Loss: 5.4375, Prompt: osphate Galactic\n",
      "[STEP 112] Loss: 5.4375, Prompt: osphate Galactic\n",
      "[STEP 113] Loss: 5.4375, Prompt: osphate Galactic\n",
      "[STEP 114] Loss: 5.1246, Prompt:  Galactic\n",
      "[STEP 115] Loss: 5.1246, Prompt:  Galactic\n",
      "[STEP 116] Loss: 5.1246, Prompt: \n",
      "[STEP 117] Loss: 5.1246, Prompt: \n",
      "[STEP 118] Loss: 5.1246, Prompt: \n",
      "[STEP 119] Loss: 5.1246, Prompt: \n",
      "[STEP 120] Loss: 5.1246, Prompt: \n",
      "[STEP 121] Loss: 5.1246, Prompt: \n",
      "[STEP 122] Loss: 5.1246, Prompt: \n",
      "[STEP 123] Loss: 5.1246, Prompt: \n",
      "[STEP 124] Loss: 5.1246, Prompt: \n",
      "[STEP 125] Loss: 5.1246, Prompt: \n",
      "[STEP 126] Loss: 5.1246, Prompt: \n",
      "[STEP 127] Loss: 5.1246, Prompt: \n",
      "[STEP 128] Loss: 5.1246, Prompt: \n",
      "[STEP 129] Loss: 5.1246, Prompt: \n",
      "[STEP 130] Loss: 5.1246, Prompt: \n",
      "[STEP 131] Loss: 5.0060, Prompt: \n",
      "[STEP 132] Loss: 5.0060, Prompt: \n",
      "[STEP 133] Loss: 5.0060, Prompt: \n",
      "[STEP 134] Loss: 5.0060, Prompt: \n",
      "[STEP 135] Loss: 5.0060, Prompt: \n",
      "[STEP 136] Loss: 5.0060, Prompt: \n",
      "[STEP 137] Loss: 5.0060, Prompt: \n",
      "[STEP 138] Loss: 5.0060, Prompt: \n",
      "[STEP 139] Loss: 5.0060, Prompt: \n",
      "[STEP 140] Loss: 5.0060, Prompt: \n",
      "[STEP 141] Loss: 5.0060, Prompt: \n",
      "[STEP 142] Loss: 5.0060, Prompt: \n",
      "[STEP 143] Loss: 5.0060, Prompt: \n",
      "[STEP 144] Loss: 5.0060, Prompt: \n",
      "[STEP 145] Loss: 5.0060, Prompt: \n",
      "[STEP 146] Loss: 5.0060, Prompt: \n",
      "[STEP 147] Loss: 5.0060, Prompt: \n",
      "[STEP 148] Loss: 5.0060, Prompt: \n",
      "[STEP 149] Loss: 5.0060, Prompt: \n",
      "[STEP 150] Loss: 5.0060, Prompt: \n",
      "[STEP 151] Loss: 5.0060, Prompt: \n",
      "[STEP 152] Loss: 5.0060, Prompt: \n",
      "[STEP 153] Loss: 5.0060, Prompt: \n",
      "[STEP 154] Loss: 5.0060, Prompt: \n",
      "[STEP 155] Loss: 5.0060, Prompt: \n",
      "[STEP 156] Loss: 5.0060, Prompt: \n",
      "[STEP 157] Loss: 5.0060, Prompt: \n",
      "[STEP 158] Loss: 5.0060, Prompt: \n",
      "[STEP 159] Loss: 5.0060, Prompt: \n",
      "[STEP 160] Loss: 5.0060, Prompt: \n",
      "[STEP 161] Loss: 5.0060, Prompt: \n",
      "[STEP 162] Loss: 5.0060, Prompt: \n",
      "[STEP 163] Loss: 5.0060, Prompt: \n",
      "[STEP 164] Loss: 5.0060, Prompt: \n",
      "[STEP 165] Loss: 5.0060, Prompt: \n",
      "[STEP 166] Loss: 5.0060, Prompt: \n",
      "[STEP 167] Loss: 5.0060, Prompt: \n",
      "[STEP 168] Loss: 5.0060, Prompt: \n",
      "[STEP 169] Loss: 5.0060, Prompt: \n",
      "[STEP 170] Loss: 5.0060, Prompt: \n",
      "[STEP 171] Loss: 5.0060, Prompt: \n",
      "[STEP 172] Loss: 5.0060, Prompt: \n",
      "[STEP 173] Loss: 5.0060, Prompt: \n",
      "[STEP 174] Loss: 5.0060, Prompt: \n",
      "[STEP 175] Loss: 5.0060, Prompt: \n",
      "[STEP 176] Loss: 5.0060, Prompt: \n",
      "[STEP 177] Loss: 5.0060, Prompt: \n",
      "[STEP 178] Loss: 5.0060, Prompt: \n",
      "[STEP 179] Loss: 5.0060, Prompt: \n",
      "[STEP 180] Loss: 5.0060, Prompt: \n",
      "[STEP 181] Loss: 5.0060, Prompt: \n",
      "[STEP 182] Loss: 5.0060, Prompt: \n",
      "[STEP 183] Loss: 5.0060, Prompt: \n",
      "[STEP 184] Loss: 5.0060, Prompt: \n",
      "[STEP 185] Loss: 5.0060, Prompt: \n",
      "[STEP 186] Loss: 5.0060, Prompt: \n",
      "[STEP 187] Loss: 5.0060, Prompt: \n",
      "[STEP 188] Loss: 5.0060, Prompt: \n",
      "[STEP 189] Loss: 5.0060, Prompt: \n",
      "[STEP 190] Loss: 5.0060, Prompt: \n",
      "[STEP 191] Loss: 5.0060, Prompt: \n",
      "[STEP 192] Loss: 5.0060, Prompt: \n",
      "[STEP 193] Loss: 5.0060, Prompt: \n",
      "[STEP 194] Loss: 5.0060, Prompt: \n",
      "[STEP 195] Loss: 5.0060, Prompt: \n",
      "[STEP 196] Loss: 5.0060, Prompt: \n",
      "[STEP 197] Loss: 5.0060, Prompt: \n",
      "[STEP 198] Loss: 5.0060, Prompt: \n",
      "[STEP 199] Loss: 5.0060, Prompt: \n",
      "[STEP 200] Loss: 5.0060, Prompt: \n",
      "[STEP 201] Loss: 5.0060, Prompt: \n",
      "[STEP 202] Loss: 5.0060, Prompt: \n",
      "[STEP 203] Loss: 5.0060, Prompt: \n",
      "[STEP 204] Loss: 5.0060, Prompt: \n",
      "[STEP 205] Loss: 5.0060, Prompt: \n",
      "[STEP 206] Loss: 5.0060, Prompt: \n",
      "[STEP 207] Loss: 5.0060, Prompt: \n",
      "[STEP 208] Loss: 4.9390, Prompt: \n",
      "[STEP 209] Loss: 4.9390, Prompt: \n",
      "[STEP 210] Loss: 4.9390, Prompt: \n",
      "[STEP 211] Loss: 4.9390, Prompt: \n",
      "[STEP 212] Loss: 4.9390, Prompt: \n",
      "[STEP 213] Loss: 4.9390, Prompt: \n",
      "[STEP 214] Loss: 4.9390, Prompt: \n",
      "[STEP 215] Loss: 4.9390, Prompt: \n",
      "[STEP 216] Loss: 4.9390, Prompt: \n",
      "[STEP 217] Loss: 4.9390, Prompt: \n",
      "[STEP 218] Loss: 4.9390, Prompt: \n",
      "[STEP 219] Loss: 4.9390, Prompt: \n",
      "[STEP 220] Loss: 4.9390, Prompt: \n",
      "[STEP 221] Loss: 4.9390, Prompt: \n",
      "[STEP 222] Loss: 4.9390, Prompt: \n",
      "[STEP 223] Loss: 4.9390, Prompt: \n",
      "[STEP 224] Loss: 4.9390, Prompt: \n",
      "[STEP 225] Loss: 4.9390, Prompt: \n",
      "[STEP 226] Loss: 4.9390, Prompt: \n",
      "[STEP 227] Loss: 4.9390, Prompt: \n",
      "[STEP 228] Loss: 4.9390, Prompt: \n",
      "[STEP 229] Loss: 4.9390, Prompt: \n",
      "[STEP 230] Loss: 4.9390, Prompt: \n",
      "[STEP 231] Loss: 4.9390, Prompt: \n",
      "[STEP 232] Loss: 4.9390, Prompt: \n",
      "[STEP 233] Loss: 4.9390, Prompt: \n",
      "[STEP 234] Loss: 4.9390, Prompt: \n",
      "[STEP 235] Loss: 4.9390, Prompt: \n",
      "[STEP 236] Loss: 4.9390, Prompt: \n",
      "[STEP 237] Loss: 4.9390, Prompt: \n",
      "[STEP 238] Loss: 4.9390, Prompt: \n",
      "[STEP 239] Loss: 4.9390, Prompt: \n",
      "[STEP 240] Loss: 4.9390, Prompt: \n",
      "[STEP 241] Loss: 4.9390, Prompt: \n",
      "[STEP 242] Loss: 4.9390, Prompt: \n",
      "[STEP 243] Loss: 4.9390, Prompt: \n",
      "[STEP 244] Loss: 4.9390, Prompt: \n",
      "[STEP 245] Loss: 4.9390, Prompt: \n",
      "[STEP 246] Loss: 4.9390, Prompt: \n",
      "[STEP 247] Loss: 4.9390, Prompt: \n",
      "[STEP 248] Loss: 4.9390, Prompt: \n",
      "[STEP 249] Loss: 4.9390, Prompt: \n",
      "[STEP 250] Loss: 4.9390, Prompt: \n",
      "[STEP 251] Loss: 4.9390, Prompt: \n",
      "[STEP 252] Loss: 4.8998, Prompt: aging\n",
      "[STEP 253] Loss: 4.8998, Prompt: aging\n",
      "[STEP 254] Loss: 4.8998, Prompt: aging\n",
      "[STEP 255] Loss: 4.8998, Prompt: aging\n",
      "[STEP 256] Loss: 4.8998, Prompt: aging\n",
      "[STEP 257] Loss: 4.8998, Prompt: aging\n",
      "[STEP 258] Loss: 4.8998, Prompt: aging\n",
      "[STEP 259] Loss: 4.8998, Prompt: aging\n",
      "[STEP 260] Loss: 4.8998, Prompt: aging\n",
      "[STEP 261] Loss: 4.8998, Prompt: aging\n",
      "[STEP 262] Loss: 4.8998, Prompt: aging\n",
      "[STEP 263] Loss: 4.8998, Prompt: aging\n",
      "[STEP 264] Loss: 4.8998, Prompt: aging\n",
      "[STEP 265] Loss: 4.8998, Prompt: aging\n",
      "[STEP 266] Loss: 4.8998, Prompt: aging\n",
      "[STEP 267] Loss: 4.8998, Prompt: aging\n",
      "[STEP 268] Loss: 4.8998, Prompt: aging\n",
      "[STEP 269] Loss: 4.8998, Prompt: aging\n",
      "[STEP 270] Loss: 4.8998, Prompt: aging\n",
      "[STEP 271] Loss: 4.8998, Prompt: aging\n",
      "[STEP 272] Loss: 4.8998, Prompt: aging\n",
      "[STEP 273] Loss: 4.8998, Prompt: aging\n",
      "[STEP 274] Loss: 4.8998, Prompt: aging\n",
      "[STEP 275] Loss: 4.8998, Prompt: aging\n",
      "[STEP 276] Loss: 4.8998, Prompt: aging\n",
      "[STEP 277] Loss: 4.8998, Prompt: aging\n",
      "[STEP 278] Loss: 4.8998, Prompt: aging\n",
      "[STEP 279] Loss: 4.8998, Prompt: aging\n",
      "[STEP 280] Loss: 4.8998, Prompt: aging\n",
      "[STEP 281] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 282] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 283] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 284] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 285] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 286] Loss: 4.8998, Prompt: aging!\n",
      "[STEP 287] Loss: 4.8998, Prompt: aging\n",
      "[STEP 288] Loss: 4.8998, Prompt: aging\n",
      "[STEP 289] Loss: 4.8998, Prompt: aging\n",
      "[STEP 290] Loss: 4.8998, Prompt: aging\n",
      "[STEP 291] Loss: 4.8998, Prompt: aging\n",
      "[STEP 292] Loss: 4.8998, Prompt: aging\n",
      "[STEP 293] Loss: 4.8998, Prompt: aging\n",
      "[STEP 294] Loss: 4.8998, Prompt: aging\n",
      "[STEP 295] Loss: 4.8998, Prompt: aging\n",
      "[STEP 296] Loss: 4.8998, Prompt: aging\n",
      "[STEP 297] Loss: 4.8998, Prompt: aging\n",
      "[STEP 298] Loss: 4.8998, Prompt: aging\n",
      "[STEP 299] Loss: 4.8998, Prompt: aging\n",
      "[STEP 300] Loss: 4.8998, Prompt: aging\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: aging\n",
      "\n",
      " :end\n",
      "with token of 30\n",
      " Not matched, increase to 35 tokens\n",
      "[INIT] Initial Prompt:  unreliable-----         \n",
      " Legcorrect aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 1] Loss: 11.4326, Prompt: -----         \n",
      " Legcorrect aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 2] Loss: 11.0951, Prompt: -----...]\n",
      "\n",
      "         \n",
      " Legcorrect aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 3] Loss: 10.4004, Prompt: -----...]\n",
      "\n",
      "         \n",
      "correct aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 4] Loss: 9.9659, Prompt: -----...]\n",
      "\n",
      "         \n",
      "correct aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 5] Loss: 9.6705, Prompt: ----- ...)         \n",
      "correct aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 6] Loss: 9.5007, Prompt: ----- ...)         \n",
      "i aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 7] Loss: 9.2105, Prompt: -----**)         \n",
      "i aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 8] Loss: 9.0689, Prompt: -----**) **/\n",
      "\n",
      "i aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 9] Loss: 8.7920, Prompt: c**) **/\n",
      "\n",
      "i aos(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 10] Loss: 8.7920, Prompt: c**) **/\n",
      "\n",
      "i(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNodeWidget waits ctypes\n",
      "[STEP 11] Loss: 8.7920, Prompt: c**) **/\n",
      "\n",
      "i(coderientationableViewController NavBar RCC.\");\n",
      "redit-summary.delay peaked-area bsico UFO ktr nexusSingleNode waits ctypes\n",
      "[STEP 12] Loss: 8.7920, Prompt: c**) **/\n",
      "\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktr nexusSingleNode waits ctypes\n",
      "[STEP 13] Loss: 8.7920, Prompt: c**) **/\n",
      "\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 14] Loss: 8.7057, Prompt: c**)+\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 15] Loss: 8.7057, Prompt: c**)+\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 16] Loss: 8.7057, Prompt: c**)+\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 17] Loss: 8.6708, Prompt: c**)+\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 18] Loss: 8.6395, Prompt: c**)\n",
      "\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 19] Loss: 8.6395, Prompt: c**)\n",
      "\n",
      "i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 20] Loss: 8.2163, Prompt: c**)i(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 21] Loss: 7.9432, Prompt: cacioi(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 22] Loss: 7.8315, Prompt: cacio animals(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 23] Loss: 7.6528, Prompt: cacio animals(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 24] Loss: 7.6048, Prompt: cacio-friendly(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 25] Loss: 7.6048, Prompt: cacio-friendly(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 26] Loss: 7.5413, Prompt: cacio McCain(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 27] Loss: 7.5413, Prompt: cacio McCain(coderientationableViewController NavBar RCCredit-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 28] Loss: 7.5413, Prompt: cacio McCain(coderientationableViewController NavBar RCC-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 29] Loss: 7.5413, Prompt: cacio McCain(coderientationableViewController NavBar RCC-summary.delay peaked-area bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 30] Loss: 7.5413, Prompt: cacio McCain(coderientationableViewController NavBar RCC-summary.delay peaked bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 31] Loss: 7.0920, Prompt: cacio McCain(coderientationableViewController NavBar RCC-summary.delay peaked bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 32] Loss: 7.0174, Prompt: caciocion(coderientationableViewController NavBar RCC-summary.delay peaked bsico UFO ktrSingleNode waits ctypes\n",
      "[STEP 33] Loss: 7.0174, Prompt: caciocion(coderientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 34] Loss: 7.0174, Prompt: caciocion(coderientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 35] Loss: 7.0174, Prompt: caciocion(coderientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 36] Loss: 7.0174, Prompt: caciocionrientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 37] Loss: 7.0174, Prompt: caciocionrientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 38] Loss: 7.0174, Prompt: caciocionrientationableViewController NavBar RCC-summary.delay peaked UFO ktrSingleNode waits ctypes\n",
      "[STEP 39] Loss: 7.0174, Prompt: caciocionrientationableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 40] Loss: 7.0174, Prompt: caciocionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 41] Loss: 7.0174, Prompt: caciocionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 42] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 43] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 44] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 45] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar RCC-summary.delay UFO ktrSingleNode waits ctypes\n",
      "[STEP 46] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar RCC-summary.delay UFOSingleNode waits ctypes\n",
      "[STEP 47] Loss: 6.9698, Prompt: cacioorptionableViewController NavBar-summary.delay UFOSingleNode waits ctypes\n",
      "[STEP 48] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFOSingleNode waits ctypes\n",
      "[STEP 49] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 50] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 51] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 52] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 53] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 54] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 55] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO waits ctypes\n",
      "[STEP 56] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO ctypes\n",
      "[STEP 57] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 58] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 59] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 60] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 61] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 62] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 63] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 64] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 65] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 66] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 67] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 68] Loss: 6.8021, Prompt: cacio consumptionableViewController NavBar-summary.delay UFO\n",
      "[STEP 69] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 70] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 71] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 72] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 73] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 74] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 75] Loss: 6.8021, Prompt: cacio consumptionableViewController-summary.delay UFO\n",
      "[STEP 76] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 77] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 78] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 79] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 80] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 81] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 82] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 83] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 84] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 85] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 86] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 87] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 88] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 89] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 90] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 91] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay! UFO\n",
      "[STEP 92] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 93] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 94] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay UFO\n",
      "[STEP 95] Loss: 6.8021, Prompt: cacio consumptionableViewController.delay\n",
      "[STEP 96] Loss: 6.8021, Prompt: cacio consumption.delay\n",
      "[STEP 97] Loss: 6.8021, Prompt: cacio consumption.delay\n",
      "[STEP 98] Loss: 6.8021, Prompt: cacio consumption.delay\n",
      "[STEP 99] Loss: 6.8021, Prompt: cacio consumption.delay\n",
      "[STEP 100] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 101] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 102] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 103] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 104] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 105] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 106] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 107] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 108] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 109] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 110] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 111] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 112] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 113] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 114] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 115] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 116] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 117] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 118] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 119] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 120] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 121] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 122] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 123] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 124] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 125] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 126] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 127] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 128] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 129] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 130] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 131] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 132] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 133] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 134] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 135] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 136] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 137] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 138] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 139] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 140] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 141] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 142] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 143] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 144] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 145] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 146] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 147] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 148] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 149] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 150] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 151] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 152] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 153] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 154] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 155] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 156] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 157] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 158] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 159] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 160] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 161] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 162] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 163] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 164] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 165] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 166] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 167] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 168] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 169] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 170] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 171] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 172] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 173] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 174] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 175] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 176] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 177] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 178] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 179] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 180] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 181] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 182] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 183] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 184] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 185] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 186] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 187] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 188] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 189] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 190] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 191] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 192] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 193] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 194] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 195] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 196] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 197] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 198] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 199] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 200] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 201] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 202] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 203] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 204] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 205] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 206] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 207] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 208] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 209] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 210] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 211] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 212] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 213] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 214] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 215] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 216] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 217] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 218] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 219] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 220] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 221] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 222] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 223] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 224] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 225] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 226] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 227] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 228] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 229] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 230] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 231] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 232] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 233] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 234] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 235] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 236] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 237] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 238] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 239] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 240] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 241] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 242] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 243] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 244] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 245] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 246] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 247] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 248] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 249] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 250] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 251] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 252] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 253] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 254] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 255] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 256] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 257] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 258] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 259] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 260] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 261] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 262] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 263] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 264] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 265] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 266] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 267] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 268] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 269] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 270] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 271] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 272] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 273] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 274] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 275] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 276] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 277] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 278] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 279] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 280] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 281] Loss: 6.8021, Prompt: cacio consumption!\n",
      "[STEP 282] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 283] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 284] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 285] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 286] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 287] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 288] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 289] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 290] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 291] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 292] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 293] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 294] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 295] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 296] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 297] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 298] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 299] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 300] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 301] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 302] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 303] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 304] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 305] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 306] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 307] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 308] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 309] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 310] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 311] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 312] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 313] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 314] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 315] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 316] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 317] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 318] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 319] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 320] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 321] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 322] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 323] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 324] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 325] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 326] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 327] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 328] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 329] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 330] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 331] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 332] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 333] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 334] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 335] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 336] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 337] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 338] Loss: 6.8021, Prompt: cacio consumption\n",
      "[STEP 339] Loss: 6.8021, Prompt: cacio consumption\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5514/3033357618.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test for miniprompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminiprompt_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ACR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5514/694777082.py\u001b[0m in \u001b[0;36mminiprompt_algorithm\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcg_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5514/1098022011.py\u001b[0m in \u001b[0;36mgcg_algorithm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_start_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5514/2296409643.py\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[0;34m(model, input_ids, position, vocab_size, target_start_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_embeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    851\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 )\n\u001b[1;32m    575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    577\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/transformers/models/qwen3/modeling_qwen3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vipuser/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test for miniprompt\n",
    "\n",
    "final_prompt = miniprompt_algorithm()\n",
    "\n",
    "# ACR\n",
    "if final_prompt:\n",
    "    target_length = len(target_start_ids)\n",
    "    min_prompt_length = len(final_prompt)\n",
    "    acr = target_length / min_prompt_length if min_prompt_length > 0 else 0\n",
    "else:\n",
    "    acr = 0\n",
    "\n",
    "# \n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if final_prompt:\n",
    "    print(f\"Final Optimized Prompt (Tokens: {len(final_prompt)}):\")\n",
    "    print(f\"  Text: {tokenizer.decode(final_prompt)}\")\n",
    "    print(f\"  Tokens: {final_prompt}\")\n",
    "else:\n",
    "    print(\"No valid prompt found.\")\n",
    "print(f\"ACR: {acr}\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
