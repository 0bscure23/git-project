{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c338462",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (0.31.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->huggingface_hub) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atasets (C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atasets (C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atasets (C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: trl in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (1.7.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: transformers>=4.46.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (4.51.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: trl in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (1.7.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: rich in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from accelerate>=0.34.0->trl) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\.conda\\envs\\advprompter\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in /home/vipuser/anaconda3/lib/python3.9/site-packages (3.6.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: packaging in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (3.3.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (1.20.3)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: trl in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: rich in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (4.51.3)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (21.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (1.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (0.31.1)\n",
      "Requirement already satisfied: pyyaml in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (6.0)\n",
      "Requirement already satisfied: psutil in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (5.8.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (2.7.0)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.3.1)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2025.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2.32.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (1.3.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (21.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate>=0.34.0->trl) (3.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2021.10.8)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.85)\n",
      "Requirement already satisfied: jinja2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: networkx in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.6.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.26.2)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.3.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.80)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate>=0.34.0->trl) (58.0.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.0->trl) (1.2.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers>=4.46.0->trl) (0.21.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers>=4.46.0->trl) (2021.8.3)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in /home/vipuser/anaconda3/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in /home/vipuser/anaconda3/lib/python3.9/site-packages (4.51.3)\n",
      "Requirement already satisfied: trl in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.17.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: fsspec in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: networkx in /home/vipuser/anaconda3/lib/python3.9/site-packages (from torch) (2.6.3)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from triton==3.3.0->torch) (58.0.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: requests in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (3.6.0)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (1.6.0)\n",
      "Requirement already satisfied: rich in /home/vipuser/anaconda3/lib/python3.9/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: psutil in /home/vipuser/anaconda3/lib/python3.9/site-packages (from accelerate>=0.34.0->trl) (5.8.0)\n",
      "Requirement already satisfied: pandas in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (1.3.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /home/vipuser/anaconda3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from fsspec->torch) (3.11.18)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.20.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (2.6.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (6.4.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch) (21.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple huggingface_hub\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple datasets\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple trl\n",
    "%pip install -i https://pypi.tuna.tsinghua.edu.cn/simple --upgrade torch transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b4dc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/vipuser/anaconda3/lib/python3.9/site-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: requests in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from huggingface_hub) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub) (3.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vipuser/anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub) (3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61c5da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37e84c4-4bd6-47de-a253-abd071568f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "748e1841-74f5-46ca-87a4-4cb81ec6ae88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "import done\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, HfFolder\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(torch.__version__)\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86332daa-8fa7-43c7-8aaf-b8ba4eb7dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face API \n"
     ]
    }
   ],
   "source": [
    "# log in\n",
    "\n",
    "# Hugging Face\n",
    "hf_token = \"\"\n",
    "\n",
    "# Hugging Face\n",
    "login(token=hf_token)\n",
    "\n",
    "# \n",
    "api = HfApi()\n",
    "token = HfFolder.get_token()\n",
    "if token:\n",
    "    print(\"Hugging Face API \")\n",
    "else:\n",
    "    print(\"Hugging Face API \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce73c82-4c37-44bb-a6c7-5fd63bfbd427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00393442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 11.99 GB\n",
      ": 0.00 GB\n",
      ": 11.99 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#  CUDA \n",
    "if torch.cuda.is_available():\n",
    "    # \n",
    "    device = torch.cuda.current_device()\n",
    "    # \n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    # \n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    # \n",
    "    free_memory = total_memory - allocated_memory\n",
    "    \n",
    "    print(f\": {total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\": {allocated_memory / 1024**3:.2f} GB\")\n",
    "    print(f\": {free_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\" CUDA \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "704c2bc7-ad51-463c-b75f-b41e8fc728c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def format_instruction_example(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    response = example[\"output\"]\n",
    "    \n",
    "    if input_text and input_text.strip() != \"\":\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{response}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "    \n",
    "    # \n",
    "    encoded = tokenizer(text, truncation=True, max_length=512)\n",
    "    \n",
    "    # \"text\"\n",
    "    encoded[\"text\"] = text\n",
    "    \n",
    "    # \n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1f6011-c6ba-4aa2-ab18-6067e66667cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\asus\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-0.6B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Map: 100%|| 8164/8164 [00:04<00:00, 1730.65 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model and dataset load done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "# model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# \n",
    "dataset = load_dataset(\"BRZ911/Medical_consultation_data_SFT\", split=\"train\")\n",
    "\n",
    "# 10%\n",
    "train_dataset = dataset.select(range(int(len(dataset) * 0.01)))\n",
    "\n",
    "# \n",
    "formatted_dataset = train_dataset.map(format_instruction_example)\n",
    "\n",
    "print('model and dataset load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7fe787c-27d9-4e84-810c-17011d3b2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": dict_keys(['instruction', 'input', 'output'])\n",
      "\n",
      ": {'instruction': '', 'input': '', 'output': ''}\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print(\":\", train_dataset.features.keys())\n",
    "\n",
    "# \n",
    "if len(train_dataset) > 0:\n",
    "    print(\"\\n:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f27d560-5f93-41c2-b603-bc71325973ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 2 :\n",
      "22.217.3.\n",
      "\n",
      " (200 tokens):\n",
      "22.217.3\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 3 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 4 :\n",
      "28\n",
      "\n",
      " (200 tokens):\n",
      "28\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 5 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "### Response:\n",
      "\n",
      "\n",
      "### Response:\n",
      "...\n",
      "\n",
      ":\n",
      "X\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 6 :\n",
      "...\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "1. ****\n",
      "2. **CCRP**CRP\n",
      "3. **XCT**\n",
      "4. ****...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "1. \n",
      "\n",
      "2. \n",
      "\n",
      "3. ...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 7 :\n",
      ".\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 8 :\n",
      ",,,\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 9 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "...\n",
      "\n",
      ":\n",
      "BEC\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 10 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "X...\n",
      "\n",
      ":\n",
      "X\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 11 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "1. ****\n",
      "\n",
      "2. ****\n",
      "\n",
      "3. ****...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 12 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 13 :\n",
      "9\n",
      "\n",
      " (200 tokens):\n",
      "9\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 14 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "### Response:\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 15 :\n",
      ",B9*6mm1424B9*6mm)\n",
      "\n",
      " (200 tokens):\n",
      "B9*6mm\n",
      "\n",
      "### Response:\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 16 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "ADHD\n",
      "\n",
      " doproton (MDD)...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 17 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "3\n",
      "\n",
      "### Response:\n",
      "3...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 18 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 19 :\n",
      "20\n",
      "\n",
      " (200 tokens):\n",
      "20\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 20 :\n",
      ",?.,??.??....\n",
      "\n",
      " (200 tokens):\n",
      "ed\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 21 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 22 :\n",
      "...\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "### Response:\n",
      "...\n",
      "\n",
      ":\n",
      "...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 23 :\n",
      "...\n",
      "\n",
      " (200 tokens):\n",
      "ADHDADHDADHDADHDADHD\n",
      "\n",
      "ADHD\n",
      "\n",
      "1. ****B12...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 24 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "1. ****\n",
      "\n",
      "2. ****...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 25 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 26 :\n",
      "1213.6,5.5.\n",
      "\n",
      " (200 tokens):\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "1. \n",
      "2. \n",
      "3. \n",
      "4. \n",
      "5. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 27 :\n",
      ":,,,.,.::CT,,...\n",
      "\n",
      " (200 tokens):\n",
      "CT\n",
      "\n",
      "CT...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      " 28 :\n",
      "\n",
      "\n",
      " (200 tokens):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "...\n",
      "\n",
      ":\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m### Instruction:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00minstruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m generated_response = generate_text(prompt)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m :\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mgenerate_text\u001b[39m\u001b[34m(prompt, max_length)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     outputs = model.generate(\n\u001b[32m     12\u001b[39m         **inputs,\n\u001b[32m     13\u001b[39m         max_length=\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs.input_ids[\u001b[32m0\u001b[39m]) + max_length, \u001b[32m2048\u001b[39m),  \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     14\u001b[39m         temperature=\u001b[32m0.0\u001b[39m,      \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     15\u001b[39m         num_beams=\u001b[32m1\u001b[39m,          \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     16\u001b[39m         do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m       \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m     20\u001b[39m generated_text = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\generation\\utils.py:2465\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[39m\n\u001b[32m   2457\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2458\u001b[39m         input_ids=input_ids,\n\u001b[32m   2459\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2460\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2461\u001b[39m         **model_kwargs,\n\u001b[32m   2462\u001b[39m     )\n\u001b[32m   2464\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2465\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2466\u001b[39m         input_ids,\n\u001b[32m   2467\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2468\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2469\u001b[39m         generation_config=generation_config,\n\u001b[32m   2470\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2471\u001b[39m         streamer=streamer,\n\u001b[32m   2472\u001b[39m         **model_kwargs,\n\u001b[32m   2473\u001b[39m     )\n\u001b[32m   2475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2476\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2477\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2478\u001b[39m         input_ids=input_ids,\n\u001b[32m   2479\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2480\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2481\u001b[39m         **model_kwargs,\n\u001b[32m   2482\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\generation\\utils.py:3434\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3432\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3434\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3436\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3437\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3438\u001b[39m     outputs,\n\u001b[32m   3439\u001b[39m     model_kwargs,\n\u001b[32m   3440\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3441\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:850\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m output_hidden_states = (\n\u001b[32m    846\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    847\u001b[39m )\n\u001b[32m    849\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    851\u001b[39m     input_ids=input_ids,\n\u001b[32m    852\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    853\u001b[39m     position_ids=position_ids,\n\u001b[32m    854\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    855\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    856\u001b[39m     use_cache=use_cache,\n\u001b[32m    857\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    858\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    859\u001b[39m     cache_position=cache_position,\n\u001b[32m    860\u001b[39m     **kwargs,\n\u001b[32m    861\u001b[39m )\n\u001b[32m    863\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    864\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = decoder_layer(\n\u001b[32m    577\u001b[39m         hidden_states,\n\u001b[32m    578\u001b[39m         attention_mask=causal_mask,\n\u001b[32m    579\u001b[39m         position_ids=position_ids,\n\u001b[32m    580\u001b[39m         past_key_value=past_key_values,\n\u001b[32m    581\u001b[39m         output_attentions=output_attentions,\n\u001b[32m    582\u001b[39m         use_cache=use_cache,\n\u001b[32m    583\u001b[39m         cache_position=cache_position,\n\u001b[32m    584\u001b[39m         position_embeddings=position_embeddings,\n\u001b[32m    585\u001b[39m         **flash_attn_kwargs,\n\u001b[32m    586\u001b[39m     )\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:286\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    274\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m     **kwargs: Unpack[FlashAttentionKwargs],\n\u001b[32m    283\u001b[39m ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n\u001b[32m    284\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    289\u001b[39m     hidden_states, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    290\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    291\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m         **kwargs,\n\u001b[32m    299\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\advprompter\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "# \n",
    "def generate_text(prompt, max_length=200):\n",
    "    \"\"\"200 tokens\"\"\"\n",
    "    # \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=min(len(inputs.input_ids[0]) + max_length, 2048),  # \n",
    "            temperature=0.0,      # \n",
    "            num_beams=1,          # \n",
    "            do_sample=False       # \n",
    "        )\n",
    "    \n",
    "    # \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):].strip()  # \n",
    "\n",
    "# 10\n",
    "for i, example in enumerate(train_dataset):\n",
    "    # \n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    \n",
    "    # \n",
    "    if input_text and input_text.strip() != \"\":\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    \n",
    "    # \n",
    "    generated_response = generate_text(prompt)\n",
    "    \n",
    "    # \n",
    "    print(f\"\\n {i+1} :\")\n",
    "    print(instruction[:100] + (\"...\" if len(instruction) > 100 else \"\"))  # \n",
    "    \n",
    "    print(f\"\\n (200 tokens):\")\n",
    "    print(generated_response[:200] + (\"...\" if len(generated_response) > 200 else \"\"))\n",
    "    \n",
    "    print(\"\\n:\")\n",
    "    print(example[\"output\"][:200] + (\"...\" if len(example[\"output\"]) > 200 else \"\"))\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45074138-25ad-44c6-b10a-f51baa405a58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nanogcg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnanogcg\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnanogcg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run, GCGConfig\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Notebook\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nanogcg'"
     ]
    }
   ],
   "source": [
    "import nanogcg\n",
    "from nanogcg import run, GCGConfig\n",
    "# Notebook\n",
    "\n",
    "# \n",
    "messages = [{\"role\": \"user\", \"content\": \"Translate this sentence: \"}]\n",
    "target = \"This is a test.\"\n",
    "\n",
    "# \n",
    "config = GCGConfig(\n",
    "    num_steps=50,\n",
    "    search_width=128,\n",
    "    topk=64,\n",
    "    n_replace=2,\n",
    "    use_prefix_cache=False  # \n",
    ")\n",
    "\n",
    "result = run(model, tokenizer, messages, target, config)\n",
    "print(f\"Best string: {result.best_string}\")\n",
    "print(f\"Best loss: {result.best_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d58f614f-064c-4724-b953-71577b5dcea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: ain) )) )) ) ... (MediaMeshPro augmentedanasitet node.) onto ... (MediaMeshPro augmentedanasitet node.) onto ... (MediaMeshPro augmentedanasitet node.) onto ... (MediaMeshPro augmentedanasitet node.) onto ... (MediaMeshPro augmented\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "optimized_string = result.best_string\n",
    "\n",
    "# \n",
    "prompt = f\"Translate this sentence: {optimized_string}\"\n",
    "\n",
    "# \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generate\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,  # \n",
    "        temperature=0.7,    # \n",
    "        top_p=0.9,          # \n",
    "        do_sample=True      # \n",
    "    )\n",
    "\n",
    "# \n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c059723c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "for param in model.parameters():\n",
    "    if param.device.type == 'cuda':\n",
    "        print(\" GPU \")\n",
    "        break\n",
    "else:\n",
    "    print(\" GPU  CPU \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea45c28-3719-47be-a7aa-3ec0d8a48382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def format_instruction_example(example):\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example[\"input\"]\n",
    "    response = example[\"output\"]\n",
    "    \n",
    "    if input_text and input_text.strip() != \"\":\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n{response}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
    "    \n",
    "    # \n",
    "    encoded = tokenizer(text, truncation=True, max_length=512)\n",
    "    \n",
    "    # \"text\"\n",
    "    encoded[\"text\"] = text\n",
    "    \n",
    "    # \n",
    "    encoded[\"labels\"] = encoded[\"input_ids\"].copy()\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "# \n",
    "formatted_dataset = train_dataset.map(format_instruction_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c4efdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import setup_chat_format\n",
    "# Set up the chat format with default 'chatml' format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c82c9c2-7b7d-4c29-9334-d3baea532324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a380971d16b749f9b2b7609544144041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/816477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 'text'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # training_args = SFTConfig(output_dir=\"/tmp\")\n",
    "    #  SFT \n",
    "    training_args = SFTConfig(\n",
    "    # \n",
    "    output_dir=\"/tmp\",\n",
    "    # \n",
    "    num_train_epochs=3,\n",
    "    #  GPU \n",
    "    per_device_train_batch_size=8,\n",
    "    # \n",
    "    gradient_accumulation_steps=4,\n",
    "    # \n",
    "    learning_rate=2e-5,\n",
    "    # \n",
    "    weight_decay=0.01,\n",
    "    # \n",
    "    warmup_steps=500,\n",
    "    #  AdamW\n",
    "    optim=\"adamw_torch\",\n",
    "    # \n",
    "    save_steps=10_000,\n",
    "    # \n",
    "    logging_steps=100,\n",
    "    #  fp16 \n",
    "    fp16=True,\n",
    ")\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=formatted_dataset,\n",
    "        args=training_args,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\": {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf5d9d-0ab3-48a3-9f05-a361427e783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:\\Users\\asus\\Desktop\\fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "save_path = r'C:\\Users\\asus\\Desktop\\fine_tuned_model'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\" {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c45603-27b3-4d15-9965-a90c8a95c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:\n",
      "GPU: 2274.25 MB\n",
      "GPU: 2276.00 MB\n",
      " CPU\n",
      " \n",
      "\n",
      "GPU:\n",
      "GPU: 0.00 MB\n",
      "GPU: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# GPU\n",
    "def print_gpu_memory():\n",
    "    \"\"\"GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "    else:\n",
    "        print(\"GPU\")\n",
    "\n",
    "# GPU\n",
    "print(\"GPU:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# \n",
    "try:\n",
    "    # CPU\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        model.to('cpu')\n",
    "        del model\n",
    "        print(\" CPU\")\n",
    "    else:\n",
    "        print(\" \")\n",
    "    \n",
    "    # \n",
    "    if 'tokenizer' in locals() or 'tokenizer' in globals():\n",
    "        del tokenizer\n",
    "        print(\" \")\n",
    "    else:\n",
    "        print(\" \")\n",
    "    \n",
    "    # GPU\n",
    "    if 'train_dataset' in locals() or 'train_dataset' in globals():\n",
    "        del train_dataset\n",
    "    if 'val_dataset' in locals() or 'val_dataset' in globals():\n",
    "        del val_dataset\n",
    "    \n",
    "    # PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # \n",
    "    gc.collect()\n",
    "    \n",
    "    # GPU\n",
    "    print(\"\\nGPU:\")\n",
    "    print_gpu_memory()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\": {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c37d6b-f2f4-484a-a674-76e83debf8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)\n",
    "\n",
    "# \n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# \n",
    "prompt = \"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# \n",
    "generate_config = {\n",
    "    \"max_length\": 100,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "# \n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, streamer=streamer, **generate_config)\n",
    "\n",
    "# streamer\n",
    "if not streamer:\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bead75-02e4-443d-a223-2b482e5b6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "target_start = \"\"\n",
    "target_start_ids = tokenizer(target_start, return_tensors=\"pt\").input_ids.squeeze().tolist()\n",
    "\n",
    "# \n",
    "n_tokens = 15          # 10\n",
    "num_steps = 150         # GCG\n",
    "k = 128                # top-k\n",
    "B = 64                 # \n",
    "sample_size = n_tokens      # token\n",
    "max_generation_length = len(target_start_ids) + 20  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3cb02-785c-430a-944c-20eb36f0d3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, target_start_ids):\n",
    "    \"\"\"\"\"\"\n",
    "    target_tensor = torch.tensor(target_start_ids, device=logits.device).long()\n",
    "    batch_size, seq_len, vocab_size = logits.shape\n",
    "    target_len = len(target_start_ids)\n",
    "    valid_positions = seq_len - target_len + 1  # \n",
    "    \n",
    "    if valid_positions <= 0:\n",
    "        return torch.tensor(0.0, device=logits.device)  # \n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for i in range(batch_size):\n",
    "        # \n",
    "        if seq_len >= target_len:\n",
    "            sub_logits = logits[i, :target_len]  # target_lentoken\n",
    "            loss = F.cross_entropy(sub_logits.view(-1, vocab_size), target_tensor)\n",
    "            total_loss += loss\n",
    "    \n",
    "    return total_loss / (batch_size * max(1, valid_positions))  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46f128-20d7-4d00-87ec-efd2df7c3ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(model, input_ids, position, vocab_size, target_start_ids):\n",
    "    \"\"\"\n",
    "    tokenone-hot\n",
    "    :return: [vocab_size]\n",
    "    \"\"\"\n",
    "    embed_layer = model.get_input_embeddings()\n",
    "    original_token = input_ids[position].item()\n",
    "    vocab_size_model = embed_layer.weight.size(0)  # \n",
    "\n",
    "    # one-hot\n",
    "    one_hot = torch.zeros(vocab_size_model, device=device)\n",
    "    one_hot[original_token] = 1.0\n",
    "    one_hot.requires_grad_()\n",
    "\n",
    "    input_embeds = embed_layer(input_ids.unsqueeze(0))  # [1, seq_len, embed_dim]\n",
    "    input_embeds[0, position] = one_hot @ embed_layer.weight  # \n",
    "\n",
    "    # ++\n",
    "    with torch.enable_grad():\n",
    "        logits = model(inputs_embeds=input_embeds).logits\n",
    "        loss = compute_loss(logits, target_start_ids)\n",
    "        loss.backward()\n",
    "\n",
    "    return one_hot.grad  # token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb80f24-bc5f-4784-9e75-50b429ab46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcg_algorithm():\n",
    "    # n_tokenstoken\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    initial_prompt = torch.randint(0, vocab_size, (n_tokens,), device=device).tolist()\n",
    "    current_prompt = initial_prompt.copy()\n",
    "    step_losses = []\n",
    "    \n",
    "    print(f\"[INIT] Initial Prompt: {tokenizer.decode(current_prompt)}\")\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # tensor\n",
    "        input_ids = torch.tensor(current_prompt, device=device).long()\n",
    "        grads = []  # [seq_len, vocab_size]\n",
    "        \n",
    "        # \n",
    "        for pos in range(n_tokens):\n",
    "            grad = calculate_gradient(model, input_ids, pos, vocab_size, target_start_ids)\n",
    "            grads.append(grad.cpu().numpy())  # CPU\n",
    "        \n",
    "        # \n",
    "        candidates = []\n",
    "        for _ in range(B):\n",
    "            new_prompt = current_prompt.copy()\n",
    "            pos = np.random.randint(n_tokens)  # \n",
    "            \n",
    "            # ktoken\n",
    "            top_k_indices = np.argsort(grads[pos])[:k]  # \n",
    "            new_token = np.random.choice(top_k_indices)  # token\n",
    "            new_prompt[pos] = new_token\n",
    "            candidates.append(new_prompt)\n",
    "        \n",
    "        # \n",
    "        best_loss = float('inf')\n",
    "        best_candidate = current_prompt\n",
    "        for cand in candidates:\n",
    "            input_ids_cand = torch.tensor(cand, device=device).long().unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids_cand).logits\n",
    "                loss = compute_loss(logits, target_start_ids)\n",
    "            \n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_candidate = cand\n",
    "        \n",
    "        # \n",
    "        current_prompt = best_candidate\n",
    "        step_losses.append(best_loss)\n",
    "        print(f\"[STEP {step + 1}] Loss: {best_loss:.4f}, Prompt: {tokenizer.decode(current_prompt)}\")\n",
    "    \n",
    "    # \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(step_losses, label=\"Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"GCG Optimization Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return current_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d093d3-9122-4569-8403-0550dd6a8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miniprompt_algorithm():\n",
    "    global n_tokens\n",
    "    global num_steps\n",
    "    running_min = 0\n",
    "    running_max = 100000\n",
    "    best = None\n",
    "    while True:\n",
    "        z = gcg_algorithm()\n",
    "        input_ids = torch.tensor(z, device=device).long().unsqueeze(0)\n",
    "        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        # ID\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # \n",
    "            # generated_output = model.generate(\n",
    "            #     input_ids,\n",
    "            #     attention_mask=attention_mask,\n",
    "            #     pad_token_id=tokenizer.eos_token_id,\n",
    "            #     max_length=len(target_start_ids) + 20,  # \n",
    "            #     min_length=len(target_start_ids),       # \n",
    "            #     eos_token_id=tokenizer.eos_token_id,\n",
    "            #     early_stopping=False\n",
    "            # )\n",
    "            generated_output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_beams=50,\n",
    "                no_repeat_ngram_size=2,\n",
    "                early_stopping=False,  # \n",
    "                temperature=0.2\n",
    "            )\n",
    "            generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "            print(f\"Generated text: {generated_text} :end\")\n",
    "            print(f\"with token of {n_tokens}\")\n",
    "            \n",
    "            # \n",
    "            if generated_text.strip() == target_start.strip():\n",
    "                running_max = n_tokens\n",
    "                n_tokens = max(1, n_tokens - 1)  # 1token\n",
    "                best = z\n",
    "                num_steps = max(50, num_steps - 10)  # \n",
    "                print(f\" Matched at {n_tokens} tokens!\")\n",
    "            else:\n",
    "                running_min = n_tokens\n",
    "                n_tokens = min(n_tokens + 5, running_max)  # 5token\n",
    "                num_steps = min(500, num_steps + 50)       # \n",
    "                print(f\" Not matched, increase to {n_tokens} tokens\")\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea3229-9a07-4304-a617-c3318e61acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for miniprompt\n",
    "\n",
    "final_prompt = miniprompt_algorithm()\n",
    "\n",
    "# ACR\n",
    "if final_prompt:\n",
    "    target_length = len(target_start_ids)\n",
    "    min_prompt_length = len(final_prompt)\n",
    "    acr = target_length / min_prompt_length if min_prompt_length > 0 else 0\n",
    "else:\n",
    "    acr = 0\n",
    "\n",
    "# \n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "if final_prompt:\n",
    "    print(f\"Final Optimized Prompt (Tokens: {len(final_prompt)}):\")\n",
    "    print(f\"  Text: {tokenizer.decode(final_prompt)}\")\n",
    "    print(f\"  Tokens: {final_prompt}\")\n",
    "else:\n",
    "    print(\"No valid prompt found.\")\n",
    "print(f\"ACR: {acr}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffa2bf-e96b-4b96-94ef-bc6656aa3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "while True:\n",
    "    # \n",
    "    user_input = input(\": \")\n",
    "    if user_input == \"\":\n",
    "        break\n",
    "\n",
    "    # \n",
    "    input_ids = tokenizer.encode(user_input, return_tensors='pt').to(device)\n",
    "    # \n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "\n",
    "    # ID\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    temp = float(0.8)\n",
    "\n",
    "    # \n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        num_beams=15,\n",
    "        no_repeat_ngram_size=2,\n",
    "        early_stopping=False,  # \n",
    "        temperature=temp,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # token\n",
    "    tokens = tokenizer.tokenize(response)\n",
    "    token_count = len(tokens)\n",
    "\n",
    "    # \n",
    "    response = response[len(user_input):].strip()\n",
    "\n",
    "    print(f\": {response}\")\n",
    "    print(f\"token: {token_count}\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09235bc-1eac-4d00-af0b-628e6a6acc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
